{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chapter07.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNpi9PGGIs7ATBV9TZGRIO5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waltechel/tensorflow-ml-nlp-tf2/blob/master/7.PRETRAIN_METHOD/chapter07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8flEnxqUunT5"
      },
      "source": [
        "# chapter07 사전학습 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtYev8SFyTPa"
      },
      "source": [
        "- 하위 문제 : 사전 학습한 가중치를 활용해 학습하고자 하는 본 문제\n",
        "- 사전 학습 : 최초로 생성된 모델의 가중치를 임의의 값으로 초기화하는 것이 아니라 사전에 학습된 가중치를 활용하는 방식이다. \n",
        "- 사전 학습한 가중치를 활용하는 방법\n",
        "  1. 특징 기반 방법\n",
        "  2. 미세 조정(fine - tunning) : 하위 문제를 위한 최소한의 가중치를 추가해서 모델을 추가로 학습하는 모델이다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qanyt9FBuwIV"
      },
      "source": [
        "## 01 버트"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb-OmmWaywz4"
      },
      "source": [
        "- 버트 : 구글 논문 `<BERT : Pre-training of Deep Bidirectional Transformers for Language UnderStanding>`에서 제안된 모델 \n",
        "- 버트의 경우 사전학습하는 모델이 양방향성을 띈다는 점이 GPT, ELMo와의 차이점이다.\n",
        "- 버트의 사전 학습 문제 중 하나인 마스크 언어 모델을 학습하기 때문이다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C_jjB2IvDrt"
      },
      "source": [
        "#### 버트의 사전 학습 문제"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2Y-aLNWzuZs"
      },
      "source": [
        "- 언어 모델\n",
        "  - 단어들의 시퀀스에 대한 확률 분포\n",
        "- 언어 모델의 목적 함수\n",
        "  - 사례 > CBOW 모델의 목적 함수\n",
        "\n",
        "  $$ logp(W_t|W_{t-c}, W_{t-c+1}, ... , W_{t-1}, W_{t+1}, ... , W_{t+c-1}, W_{t+c} ) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6K-qzJ7ovHpL"
      },
      "source": [
        "#### 버트의 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpNf6vyE0vma"
      },
      "source": [
        "- 트랜스포머 모델의 전체를 사용하는 것이 아니라 인코더 부분만 사용해 모델을 학습한다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCKMgb0kvJkV"
      },
      "source": [
        "#### 버트의 미세 조정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK3W3NU90_W9"
      },
      "source": [
        "- 사전 학습한 BERT 모델을 MNLI, NER, SQuAD 라는 자연어 처리 하위 문제에 미세 조정할 수 있었다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G_dW7q41JqV"
      },
      "source": [
        "#### 이후 과제\n",
        "- 언어적 용인 가능성 : 그는 한번도 고향에 갔다 -> 불가능 판정\n",
        "- 자연어 추론\n",
        "  - 전제 : 버트를 이용한 자연어 처리를 수행하는 연구자가 사무실에 있다\n",
        "  - 가설 : 연구자는 딥러닝을 알지 못한다\n",
        "  - 답 : 모순\n",
        "- 유사도 예측\n",
        "  - 입력 1 : 버트와 트랜스포머 중에 무엇을 먼저 알아야 할까요?\n",
        "  - 입력 2 : Bert와 Transformer 중에 무엇을 먼저 알아야 할까요?\n",
        "  - 답 : 유사\n",
        "- 감정 분석\n",
        "  - 입력 : 나는 이 영화를 용서한다\n",
        "  - 답 : 부정\n",
        "- 개체명 인식 \n",
        "  - 입력 : 단백질은 단 한 가지 아미노산이라도 부족하면 합성되지 않는다\n",
        "  - 답 : 단백질(BIO), 아미노산(BIO)\n",
        "- 기계독해\n",
        "  - 입력 : ~~~~~ 수많은 문장, 문서 ~~~~~~\n",
        "  - 질문 : 이순신의 외가는?\n",
        "  - 답 : 아산"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Elowo6GpvMbv"
      },
      "source": [
        "## 02 버트를 활용한 미세 조정 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXtW4Uaa2G42"
      },
      "source": [
        "- 허깅페이스의 라이브러리를 활용하기 위해 트랜스포머 설치한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWCfzAp72MgV"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_o9PW7X2nh7"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yY5-X8R3JAI"
      },
      "source": [
        "- 트랜스포머는 한번 설치후에 런타임을 재시작해주어야 한다.\n",
        "- 재시작하는 것은 설치를 했으니까 설치가 제대로 먹도록 만들어주는 것이다. 세션을 자르는 것과는 다르다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVPkA5F2vdwp"
      },
      "source": [
        "#### 버트 파일 불러오기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPrB0veM2VD3"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from transformers import *\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dWHdZil2bzt"
      },
      "source": [
        "# 시각화\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string], '')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bE43kN_P6YS9"
      },
      "source": [
        "#random seed 고정\n",
        "tf.random.set_seed(1234)\n",
        "np.random.seed(1234)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 3\n",
        "VALID_SPLIT = 0.2\n",
        "MAX_LEN = 39 # EDA에서 추출된 Max Length\n",
        "DATA_IN_PATH = 'data_in/KOR'\n",
        "DATA_OUT_PATH = \"data_out/KOR\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSKCH5uJ20eF"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", cache_dir='bert_ckpt', do_lower_case=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-aX_6GGvf-J"
      },
      "source": [
        "#### 버트 문장 전처리 정리하기 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5zvUJuK3ksu"
      },
      "source": [
        "- 문장을 토크나이징한다\n",
        "- 토큰의 시작점에 [cls] 토큰, 토큰의 마지막에 [sep] 토큰을 붙인다.\n",
        "- 각 토큰을 인덱스로 반환한다\n",
        "- 최대 길이에 맞추어 패딩을 진행하거나, 글자를 잘라낸다\n",
        "- return_attention_mask 기능을 통해 어텐션 마스크를 생성한다\n",
        "- 토큰 타입은 문장이 1개일 경우 0으로, 문장이 2개일 경우 0과 1로 구분해서 생성한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRT-tPoC42Lw"
      },
      "source": [
        "# Bert Tokenizer\n",
        "\n",
        "# 참조: https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus\n",
        "\n",
        "def bert_tokenizer(sent, MAX_LEN):\n",
        "    \n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "        text = sent,\n",
        "        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "        max_length = MAX_LEN,           # Pad & truncate all sentences.\n",
        "        pad_to_max_length = True,\n",
        "        return_attention_mask = True   # Construct attn. masks.\n",
        "        \n",
        "    )\n",
        "    \n",
        "    input_id = encoded_dict['input_ids']\n",
        "    attention_mask = encoded_dict['attention_mask'] # And its attention mask (simply differentiates padding from non-padding).\n",
        "    token_type_id = encoded_dict['token_type_ids'] # differentiate two sentences\n",
        "    \n",
        "    return input_id, attention_mask, token_type_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g86Hv7qCvi98"
      },
      "source": [
        "### 버트를 활용한 한국어 텍스트 분류 모델 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3sU6wme4pea"
      },
      "source": [
        "#### 네이버 영화 리뷰 데이터 전처리 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP8Zoovb3dsA"
      },
      "source": [
        "test_sentence = \"안녕하세요, 반갑습니다.\"\n",
        "\n",
        "encode = tokenizer.encode(test_sentence)\n",
        "token_print = [tokenizer.decode(token) for token in encode]\n",
        "\n",
        "print(encode)\n",
        "print(token_print)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMLIhm5c3XgY"
      },
      "source": [
        "kor_encode = tokenizer.encode(\"안녕하세요, 반갑습니다\")\n",
        "eng_encode = tokenizer.encode(\"Hello world\")\n",
        "kor_decode = tokenizer.decode(kor_encode)\n",
        "eng_decode = tokenizer.decode(eng_encode)\n",
        "\n",
        "print(kor_encode)\n",
        "# [101, 9521, 118741, 35506, 24982, 48549, 117, 9321, 118610, 119081, 48345, 102]\n",
        "print(eng_encode)\n",
        "# [101, 31178, 11356, 102]\n",
        "print(kor_decode)\n",
        "# [CLS] 안녕하세요, 반갑습니다 [SEP]\n",
        "print(eng_decode)\n",
        "# [CLS] Hello world [SEP]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lNF7_I949tm"
      },
      "source": [
        "- 해당 다국어 토크나이저와 encode_plus 기능을 활용해 영화 리뷰 데이터를 전처리해보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTPBGojW6GaF"
      },
      "source": [
        "- \\data_in\\KOR\\naver_movie 경로에 데이터를 갖다 놓는다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reBpIr9d5Nwh"
      },
      "source": [
        "# 데이터 전처리 준비\n",
        "DATA_TRAIN_PATH = os.path.join(DATA_IN_PATH, \"naver_movie\", \"ratings_train.txt\")\n",
        "DATA_TEST_PATH = os.path.join(DATA_IN_PATH, \"naver_movie\", \"ratings_test.txt\")\n",
        "\n",
        "train_data = pd.read_csv(DATA_TRAIN_PATH, header = 0, delimiter = '\\t', quoting = 3)\n",
        "train_data = train_data.dropna()\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0XhQVKP5Ek0"
      },
      "source": [
        "# train_data = train_data[:1000] # for test\n",
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "token_type_ids = []\n",
        "train_data_labels = []\n",
        "\n",
        "for train_sent, train_label in tqdm(zip(train_data[\"document\"], train_data[\"label\"]), total=len(train_data)):\n",
        "    try:\n",
        "        input_id, attention_mask, token_type_id = bert_tokenizer(train_sent, MAX_LEN)\n",
        "        \n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "        token_type_ids.append(token_type_id)\n",
        "        train_data_labels.append(train_label)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(train_sent)\n",
        "        pass\n",
        "\n",
        "train_movie_input_ids = np.array(input_ids, dtype=int)\n",
        "train_movie_attention_masks = np.array(attention_masks, dtype=int)\n",
        "train_movie_type_ids = np.array(token_type_ids, dtype=int)\n",
        "train_movie_inputs = (train_movie_input_ids, train_movie_attention_masks, train_movie_type_ids)\n",
        "\n",
        "train_data_labels = np.asarray(train_data_labels, dtype=np.int32) #레이블 토크나이징 리스트\n",
        "\n",
        "print(\"# sents: {}, # labels: {}\".format(len(train_movie_input_ids), len(train_data_labels)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf2P2hjf6tl6"
      },
      "source": [
        "- 전체 문장 중 하나를 추출해서 출력해본다.\n",
        "- 결괏값을 보면 각 문장이 단어의 부분 단위로 쪼개져서 각 숫자에 인덱스 처리된 것을 볼 수 있으며, 해당 문장은 전처리를 진행할 때 최대 길이보다 작기 때문에 뒤에 0으로 패딩된 것을 알 수 있따."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiiAHTdL6pbx"
      },
      "source": [
        "# 최대 길이: 39\n",
        "input_id = train_movie_input_ids[1]\n",
        "attention_mask = train_movie_attention_masks[1]\n",
        "token_type_id = train_movie_type_ids[1]\n",
        "\n",
        "print(input_id)\n",
        "print(attention_mask)\n",
        "print(token_type_id)\n",
        "print(tokenizer.decode(input_id))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNyUEXQGvnS4"
      },
      "source": [
        "#### 네이버 영화 리뷰 모델 학습\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DxBCVyE7B5b"
      },
      "source": [
        "- 학습을 준비하기 위해 버트 분류 클래스를 구성하고, 최적화, 손실값 및 정확도를 선언하고 학습을 진행하기 위한 compile을 진행한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzf0lmhr7KQE"
      },
      "source": [
        "class TFBertClassifier(tf.keras.Model):\n",
        "    def __init__(self, model_name, dir_path, num_class):\n",
        "        super(TFBertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\n",
        "        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n",
        "        self.classifier = tf.keras.layers.Dense(num_class, \n",
        "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range), \n",
        "                                                name=\"classifier\")\n",
        "        \n",
        "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
        "        \n",
        "        #outputs 값: # sequence_output, pooled_output, (hidden_states), (attentions)\n",
        "        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        pooled_output = outputs[1] \n",
        "        pooled_output = self.dropout(pooled_output, training=training)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# 감성 분석 데이터에서는 정답의 수는 2개로 나뉜다. \n",
        "cls_model = TFBertClassifier(model_name='bert-base-multilingual-cased',\n",
        "                                  dir_path='bert_ckpt',\n",
        "                                  num_class=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvQS7rc67qd5"
      },
      "source": [
        "- 아담 최적화, 크로스엔트로피 손실값 측정, 모델의 정확도를 측정한다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aGqvY_y7y-k"
      },
      "source": [
        "# 학습 준비하기\n",
        "optimizer = tf.keras.optimizers.Adam(3e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "cls_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnFpf0GY713B"
      },
      "source": [
        "- gpu 학습이 필요한 부분이다.\n",
        "- ~~data_out 폴더를 만들어야 한다.~~ 만들지 않아도 된다\n",
        "- 검증 데이터를 기반으로 해당 데이터는 약 3에폭 학습했을 때 좋은 성능을 내는 것을 확인할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubR1PLjs76TA"
      },
      "source": [
        "model_name = \"tf2_bert_naver_movie\"\n",
        "\n",
        "# overfitting을 막기 위한 earlystop 추가\n",
        "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001,patience=2)\n",
        "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n",
        "# patience: no improvment epochs (patience = 1, 1번 이상 상승이 없으면 종료)\\\n",
        "\n",
        "checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create path if exists\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
        "else:\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
        "    \n",
        "cp_callback = ModelCheckpoint(\n",
        "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "# 학습과 eval 시작\n",
        "history = cls_model.fit(train_movie_inputs, train_data_labels, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,\n",
        "                    validation_split = VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])\n",
        "\n",
        "#steps_for_epoch\n",
        "\n",
        "print(history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqKkZXINvqiK"
      },
      "source": [
        "#### 네이버 영화 리뷰 모델 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZER0Irp9NgR"
      },
      "source": [
        "test_data = pd.read_csv(DATA_TEST_PATH, header = 0, delimiter = '\\t', quoting = 3)\n",
        "test_data = test_data.dropna()\n",
        "test_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0_sJ9Fi9rFK"
      },
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "token_type_ids = []\n",
        "test_data_labels = []\n",
        "\n",
        "for test_sent, test_label in tqdm(zip(test_data[\"document\"], test_data[\"label\"])):\n",
        "    try:\n",
        "        input_id, attention_mask, token_type_id = bert_tokenizer(test_sent, MAX_LEN)\n",
        "\n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "        token_type_ids.append(token_type_id)\n",
        "        test_data_labels.append(test_label)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(test_sent)\n",
        "        pass\n",
        "\n",
        "test_movie_input_ids = np.array(input_ids, dtype=int)\n",
        "test_movie_attention_masks = np.array(attention_masks, dtype=int)\n",
        "test_movie_type_ids = np.array(token_type_ids, dtype=int)\n",
        "test_movie_inputs = (test_movie_input_ids, test_movie_attention_masks, test_movie_type_ids)\n",
        "\n",
        "test_data_labels = np.asarray(test_data_labels, dtype=np.int32) #레이블 토크나이징 리스트\n",
        "\n",
        "print(\"num sents, labels {}, {}\".format(len(test_movie_input_ids), len(test_data_labels)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhghD5MK9wWX"
      },
      "source": [
        "results = cls_model.evaluate(test_movie_inputs, test_data_labels, batch_size=1024)\n",
        "print(\"test loss, test acc: \", results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMk5BPb_v29t"
      },
      "source": [
        "### 버트를 활용한 한국어 자연어 추론 모델\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQfwRBHF-ewI"
      },
      "source": [
        "- 카카오브레인에서 공개한 새로운 한국어 벤치마크 셋인 KorNLI 데이터\n",
        "- 카카오브레인 사랑합니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBYxPoc5v57A"
      },
      "source": [
        "#### KorNLI 데이터 분석\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KdvXyLz-k5T"
      },
      "source": [
        "- KorNLI 깃허브 저장소 \n",
        "  - https://github.com/kakaobrain/KorNLUDatasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoGVoTbg-jve"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import BertTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRFjGmEM-y83"
      },
      "source": [
        "DATA_IN_PATH = './data_in/KOR'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnIGG7dW-_LR"
      },
      "source": [
        "- 판다스를 통해 파일을 불러와보자\n",
        "- 데이터프레임의 head(10)을 보면 두 개의 값 sentence1과 sentence2는 각각 첫번째 문장과 두번째 문장을 의미하며, gold_label은 해당 문장의 정답을 의미한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwyN8qrr_B2z"
      },
      "source": [
        "TRAIN_XNLI_DF = os.path.join(DATA_IN_PATH, 'KorNLI', 'multinli.train.ko.tsv')\n",
        "\n",
        "multinli_data = pd.read_csv(TRAIN_XNLI_DF, sep='\\t', error_bad_lines=False)\n",
        "multinli_data.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3hNLHza_YiQ"
      },
      "source": [
        "print('전체 multinli_data 개수: {}'.format(len(multinli_data)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG238aov_d5b"
      },
      "source": [
        "train_data = pd.concat([multinli_data, snli_data], axis=0)\n",
        "train_data.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fD7NH30_kKD"
      },
      "source": [
        "- 데이터의 개수가 무려 935,646이 된다.\n",
        "- 지금보다는 더 좋은 장비가 필요하다. 그리고 분석이나 학습 시 시간이 많이 필요하게 되었다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLGwol_8_hgK"
      },
      "source": [
        "print('전체 train_data 개수: {}'.format(len(train_data)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUnaQUmb_tRm"
      },
      "source": [
        "- 문장 1과 문장 2를 합친다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtMQJWY9_xBS"
      },
      "source": [
        "train_set = pd.Series(train_data['sentence1'].tolist() + train_data['sentence2'].tolist()).astype(str)\n",
        "train_set.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8q3WAhj_3dZ"
      },
      "source": [
        "- 합친 문장의 데이터의 개수 : 1,871,292\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqK1D0eU_6Z0"
      },
      "source": [
        "print('전체 문장 데이터의 개수: {}'.format(len(train_set)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zuNJQoeABfj"
      },
      "source": [
        "- 데이터 파악을 위해 중복된 문장도 찾아보고\n",
        "- 시각화를 해본다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw2EBg64AFQr"
      },
      "source": [
        "print('유일한 총 문장 수 : {}'.format(len(np.unique(train_set))))\n",
        "print('반복해서 나타나는 문장의 수: {}'.format(np.sum(train_set.value_counts() > 1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPBOcw5wAHX1"
      },
      "source": [
        "# 그래프에 대한 이미지 사이즈 선언\n",
        "# figsize: (가로, 세로) 형태의 튜플로 입력\n",
        "plt.figure(figsize=(12, 5))\n",
        "# 히스토그램 선언\n",
        "# bins: 히스토그램 값들에 대한 버켓 범위\n",
        "# range: x축 값의 범위\n",
        "# alpha: 그래프 색상 투명도\n",
        "# color: 그래프 색상\n",
        "# label: 그래프에 대한 라벨\n",
        "plt.hist(train_set.value_counts(), bins=50, alpha=0.5, color= 'r', label='word')\n",
        "plt.yscale('log', nonposy='clip')\n",
        "# 그래프 제목\n",
        "plt.title('Log-Histogram of sentence appearance counts')\n",
        "# 그래프 x 축 라벨\n",
        "plt.xlabel('Number of occurrences of sentence')\n",
        "# 그래프 y 축 라벨\n",
        "plt.ylabel('Number of sentence')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipf2nYJ5AJeI"
      },
      "source": [
        "print('중복 최대 개수: {}'.format(np.max(train_set.value_counts())))\n",
        "print('중복 최소 개수: {}'.format(np.min(train_set.value_counts())))\n",
        "print('중복 평균 개수: {:.2f}'.format(np.mean(train_set.value_counts())))\n",
        "print('중복 표준편차: {:.2f}'.format(np.std(train_set.value_counts())))\n",
        "print('중복 중간길이: {}'.format(np.median(train_set.value_counts())))\n",
        "# 사분위의 대한 경우는 0~100 스케일로 되어있음\n",
        "print('제 1 사분위 중복: {}'.format(np.percentile(train_set.value_counts(), 25)))\n",
        "print('제 3 사분위 중복: {}'.format(np.percentile(train_set.value_counts(), 75)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MElms52AQCn"
      },
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "# 박스플롯 생성\n",
        "# 첫번째 파라메터: 여러 분포에 대한 데이터 리스트를 입력\n",
        "# labels: 입력한 데이터에 대한 라벨\n",
        "# showmeans: 평균값을 마크함\n",
        "\n",
        "plt.boxplot([train_set.value_counts()],\n",
        "             labels=['counts'],\n",
        "             showmeans=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltRu0ip3AiMV"
      },
      "source": [
        "- 버트 토크나이저를 호출한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKbPYV9qAafw"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", cache_dir='bert_ckpt', do_lower_case=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OpnXwAGAncf"
      },
      "source": [
        "train_length = train_set.apply(len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnR--GP_Ap53"
      },
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "plt.hist(train_word_counts, bins=50, range=[0, 50], facecolor='r', density=True, label='train')\n",
        "plt.title('Distribution of word count in sentence', fontsize=15)\n",
        "plt.legend()\n",
        "plt.xlabel('Number of words', fontsize=15)\n",
        "plt.ylabel('Probability', fontsize=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq9yRHpnAtGu"
      },
      "source": [
        "print('문장 단어 개수 최대 값: {}'.format(np.max(train_word_counts)))\n",
        "print('문장 단어 개수 평균 값: {:.2f}'.format(np.mean(train_word_counts)))\n",
        "print('문장 단어 개수 표준편차: {:.2f}'.format(np.std(train_word_counts)))\n",
        "print('문장 단어 개수 중간 값: {}'.format(np.median(train_word_counts)))\n",
        "print('문장 단어 개수 제 1 사분위: {}'.format(np.percentile(train_word_counts, 25)))\n",
        "print('문장 단어 개수 제 3 사분위: {}'.format(np.percentile(train_word_counts, 75)))\n",
        "print('문장 단어 개수 99 퍼센트: {}'.format(np.percentile(train_word_counts, 99)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsiCAA0VAzXS"
      },
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.boxplot(train_word_counts,\n",
        "             labels=['counts'],\n",
        "             showmeans=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wj9KBgZBNJe"
      },
      "source": [
        "- 워드 클라우드까지 확인해본다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBVh9-CPBDom"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "font_path = os.path.join(DATA_IN_PATH, 'NanumGothic.ttf')\n",
        "cloud = WordCloud(font_path = font_path, width=800, height=600).generate(\" \".join(train_set.astype(str)))\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(cloud)\n",
        "plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4io_D8gBTT9"
      },
      "source": [
        "fig, axe = plt.subplots(ncols=1)\n",
        "fig.set_size_inches(10, 3)\n",
        "sns.countplot(train_data['gold_label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHb4hdmev89o"
      },
      "source": [
        "#### KorNLI 데이터 전처리\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfGf_DIYCBKM"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q09mB22vCI_z"
      },
      "source": [
        "# 시각화\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string], '')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ooVcsCjCSCc"
      },
      "source": [
        "#random seed 고정\n",
        "\n",
        "tf.random.set_seed(1234)\n",
        "np.random.seed(1234)\n",
        "\n",
        "# BASE PARAM\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 3\n",
        "MAX_LEN = 24 * 2 # Average total * 2\n",
        "\n",
        "DATA_IN_PATH = './data_in/KOR'\n",
        "DATA_OUT_PATH = \"./data_out/KOR\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGCQ7o5vC6R9"
      },
      "source": [
        "# Load Train dataset\n",
        "\n",
        "TRAIN_SNLI_DF = os.path.join(DATA_IN_PATH, 'KorNLI', 'snli_1.0_train.kor.tsv')\n",
        "TRAIN_XNLI_DF = os.path.join(DATA_IN_PATH, 'KorNLI', 'multinli.train.ko.tsv')\n",
        "DEV_XNLI_DF = os.path.join(DATA_IN_PATH, 'KorNLI', 'xnli.dev.ko.tsv')\n",
        "\n",
        "train_data_snli = pd.read_csv(TRAIN_SNLI_DF, header=0, delimiter = '\\t', quoting = 3)\n",
        "train_data_xnli = pd.read_csv(TRAIN_XNLI_DF, header=0, delimiter = '\\t', quoting = 3)\n",
        "dev_data_xnli = pd.read_csv(DEV_XNLI_DF, header=0, delimiter = '\\t', quoting = 3)\n",
        "\n",
        "train_data_snli_xnli = train_data_snli.append(train_data_xnli)\n",
        "train_data_snli_xnli = train_data_snli_xnli.dropna()\n",
        "train_data_snli_xnli = train_data_snli_xnli.reset_index()\n",
        "\n",
        "dev_data_xnli = dev_data_xnli.dropna()\n",
        "\n",
        "print(\"Total # dataset: train - {}, dev - {}\".format(len(train_data_snli_xnli), len(dev_data_xnli)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoUYr3huDWpZ"
      },
      "source": [
        "- Bert Tokenizer와는 차이가 있다고 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvIusDKbDN3M"
      },
      "source": [
        "# Bert Tokenizer\n",
        "\n",
        "# 참조: https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", cache_dir='bert_ckpt', do_lower_case=False)\n",
        "\n",
        "def bert_tokenizer_v2(sent1, sent2, MAX_LEN):\n",
        "    \n",
        "    # For Two setenece input\n",
        "    \n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "        text = sent1,\n",
        "        text_pair = sent2,\n",
        "        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "        max_length = MAX_LEN,           # Pad & truncate all sentences.\n",
        "        pad_to_max_length = True,\n",
        "        return_attention_mask = True   # Construct attn. masks.\n",
        "        \n",
        "    )\n",
        "    \n",
        "    input_id = encoded_dict['input_ids']\n",
        "    attention_mask = encoded_dict['attention_mask'] # And its attention mask (simply differentiates padding from non-padding).\n",
        "    token_type_id = encoded_dict['token_type_ids'] # differentiate two sentences\n",
        "    \n",
        "    return input_id, attention_mask, token_type_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF1NXQcyDRzP"
      },
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "token_type_ids = []\n",
        "\n",
        "for sent1, sent2 in zip(train_data_snli_xnli['sentence1'], train_data_snli_xnli['sentence2']):\n",
        "    try:\n",
        "        input_id, attention_mask, token_type_id = bert_tokenizer_v2(sent1, sent2, MAX_LEN)\n",
        "\n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "        token_type_ids.append(token_type_id)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(sent1, sent2)\n",
        "        pass\n",
        "    \n",
        "train_snli_xnli_input_ids = np.array(input_ids, dtype=int)\n",
        "train_snli_xnli_attention_masks = np.array(attention_masks, dtype=int)\n",
        "train_snli_xnli_type_ids = np.array(token_type_ids, dtype=int)\n",
        "train_snli_xnli_inputs = (train_snli_xnli_input_ids, train_snli_xnli_attention_masks, train_snli_xnli_type_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uESWilEZDmJT"
      },
      "source": [
        "- 이전과 같이 패딩된 것을 알 수 있다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WCm98tBDolS"
      },
      "source": [
        "input_id = train_snli_xnli_input_ids[2]\n",
        "attention_mask = train_snli_xnli_attention_masks[2]\n",
        "token_type_id = train_snli_xnli_type_ids[2]\n",
        "\n",
        "print(input_id)\n",
        "print(attention_mask)\n",
        "print(token_type_id)\n",
        "print(tokenizer.decode(input_id))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wighON9nD59d"
      },
      "source": [
        "# 토크나이저를 제외하고는 5장에서 처리한 방식과 유사하게 접근\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "token_type_ids = []\n",
        "\n",
        "for sent1, sent2 in zip(dev_data_xnli['sentence1'], dev_data_xnli['sentence2']):\n",
        "    try:\n",
        "        input_id, attention_mask, token_type_id = bert_tokenizer_v2(sent1, sent2, MAX_LEN)\n",
        "\n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "        token_type_ids.append(token_type_id)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(sent1, sent2)\n",
        "        pass\n",
        "    \n",
        "dev_xnli_input_ids = np.array(input_ids, dtype=int)\n",
        "dev_xnli_attention_masks = np.array(attention_masks, dtype=int)\n",
        "dev_xnli_type_ids = np.array(token_type_ids, dtype=int)\n",
        "dev_xnli_inputs = (dev_xnli_input_ids, dev_xnli_attention_masks, dev_xnli_type_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6QT7iiyD9Tb"
      },
      "source": [
        "# Label을 Netural, Contradiction, Entailment 에서 숫자 형으로 변경한다.\n",
        "label_dict = {\"entailment\": 0, \"contradiction\": 1, \"neutral\": 2}\n",
        "def convert_int(label):\n",
        "    num_label = label_dict[label]    \n",
        "    return num_label\n",
        "\n",
        "train_data_snli_xnli[\"gold_label_int\"] = train_data_snli_xnli[\"gold_label\"].apply(convert_int)\n",
        "train_data_labels = np.array(train_data_snli_xnli['gold_label_int'], dtype=int)\n",
        "\n",
        "dev_data_xnli[\"gold_label_int\"] = dev_data_xnli[\"gold_label\"].apply(convert_int)\n",
        "dev_data_labels = np.array(dev_data_xnli['gold_label_int'], dtype=int)\n",
        "\n",
        "print(\"# train labels: {}, #dev labels: {}\".format(len(train_data_labels), len(dev_data_labels)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IhLbIEbwCur"
      },
      "source": [
        "#### KorNLI 모델 학습\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJpb_dIuEHJ9"
      },
      "source": [
        "- 이전과 거의 비슷한데 다만 현재 값은 이진 분류가 아닌 다중 분류이기 떄문에 num_labels의 값을 라벨의 수만큼 변환한다는 차이가 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlGeBgBtEFFt"
      },
      "source": [
        "class TFBertClassifier(tf.keras.Model):\n",
        "    def __init__(self, model_name, dir_path, num_class):\n",
        "        super(TFBertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\n",
        "        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n",
        "        self.classifier = tf.keras.layers.Dense(num_class, \n",
        "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range), \n",
        "                                                name=\"classifier\")\n",
        "        \n",
        "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
        "        \n",
        "        #outputs 값: # sequence_output, pooled_output, (hidden_states), (attentions)\n",
        "        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        pooled_output = outputs[1] \n",
        "        pooled_output = self.dropout(pooled_output, training=training)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "cls_model = TFBertClassifier(model_name='bert-base-multilingual-cased',\n",
        "                                  dir_path='bert_ckpt',\n",
        "                                  num_class=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWyWxrjTEf7s"
      },
      "source": [
        " - 아담, 크로스엔트로피, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaNo355QESpZ"
      },
      "source": [
        "# 학습 준비하기\n",
        "optimizer = tf.keras.optimizers.Adam(3e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "cls_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fjrb-IaNEkNs"
      },
      "source": [
        "#학습 진행하기\n",
        "model_name = \"tf2_KorNLI\"\n",
        "\n",
        "# overfitting을 막기 위한 ealrystop 추가\n",
        "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001,patience=2)\n",
        "checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create path if exists\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
        "else:\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
        "    \n",
        "cp_callback = ModelCheckpoint(\n",
        "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "# 학습과 eval 시작\n",
        "history = cls_model.fit(train_snli_xnli_inputs, train_data_labels, epochs=NUM_EPOCHS,\n",
        "            validation_data = (dev_xnli_inputs, dev_data_labels),\n",
        "            batch_size=BATCH_SIZE, callbacks=[earlystop_callback, cp_callback])\n",
        "\n",
        "#steps_for_epoch\n",
        "print(history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxaG2lA_Ep1T"
      },
      "source": [
        "plot_graphs(history, 'accuracy')\n",
        "plot_graphs(history, 'loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMa7nNO6wEfB"
      },
      "source": [
        "#### KorNLI 모델 테스트\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LgTd3B3E1Ry"
      },
      "source": [
        "- 학습된 모델을 활용해 테스트를 진행해보자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz5CNsHxE4KX"
      },
      "source": [
        "# Load Test dataset\n",
        "TEST_XNLI_DF = os.path.join(DATA_IN_PATH, 'KorNLI', 'xnli.test.ko.tsv')\n",
        "\n",
        "test_data_xnli = pd.read_csv(TEST_XNLI_DF, header=0, delimiter = '\\t', quoting = 3)\n",
        "test_data_xnli = test_data_xnli.dropna()\n",
        "test_data_xnli.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEhaFM4-E9AV"
      },
      "source": [
        "# Test set도 똑같은 방법으로 구성한다.\n",
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "token_type_ids = []\n",
        "\n",
        "for sent1, sent2 in zip(test_data_xnli['sentence1'], test_data_xnli['sentence2']):\n",
        "    \n",
        "    try:\n",
        "        input_id, attention_mask, token_type_id = bert_tokenizer_v2(sent1, sent2, MAX_LEN)\n",
        "\n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "        token_type_ids.append(token_type_id)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(sent1, sent2)\n",
        "        pass\n",
        "    \n",
        "    \n",
        "test_xnli_input_ids = np.array(input_ids, dtype=int)\n",
        "test_xnli_attention_masks = np.array(attention_masks, dtype=int)\n",
        "test_xnli_type_ids = np.array(token_type_ids, dtype=int)\n",
        "test_xnli_inputs = (test_xnli_input_ids, test_xnli_attention_masks, test_xnli_type_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6naBbQqFBYS"
      },
      "source": [
        "test_data_xnli[\"gold_label_int\"] = test_data_xnli[\"gold_label\"].apply(convert_int)\n",
        "test_data_xnli_labels = np.array(test_data_xnli['gold_label_int'], dtype=int)\n",
        "\n",
        "print(\"# sents: {}, # labels: {}\".format(len(test_xnli_input_ids), len(test_data_xnli_labels)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELyOyYbIFDCH"
      },
      "source": [
        "results = cls_model.evaluate(test_xnli_inputs, test_data_xnli_labels, batch_size=512)\n",
        "print(\"test loss, test acc: \", results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIKqUVjBwIKD"
      },
      "source": [
        "### 버트를 활용한 한국어 개체명 인식 모델\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cba4j0DlFEE6"
      },
      "source": [
        "- 개체명 인식은 문맥을 파악해서 인명, 기관명, 지명 등과 같은 문장 또는 문서에서 특정한 의미를 가지고 있는 단어 또는 어구를 인식하는 과정을 의미한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp8riRPzwLrd"
      },
      "source": [
        "#### NER 데이터 분석\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGTnnjiAFVJ7"
      },
      "source": [
        "- 데이터는 Naver NLP Challenge 2018의 개체명 인식 데이터를 활용해보자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gGvxcpEFawK"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import BertTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H76Rm53wNgU"
      },
      "source": [
        "#### 데이터 불러오기\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbNdkSDKFgJo"
      },
      "source": [
        "- 이것도 NER 폴더를 넣어야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XkBFksGFctg"
      },
      "source": [
        "# 데이터 불러오기\n",
        "DATA_IN_PATH = 'data_in/KOR'\n",
        "DATA_TRAIN_PATH = os.path.join(DATA_IN_PATH, \"NER\", \"train.tsv\")\n",
        "DATA_TEST_PATH = os.path.join(DATA_IN_PATH, \"NER\", \"test.tsv\")\n",
        "DATA_LABEL_PATH = os.path.join(DATA_IN_PATH, \"NER\", \"label.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpZgDTjbFqj2"
      },
      "source": [
        "- 문장 분석을 위해 데이터의 위치를 지정한 후, 학습 데이터와 테스트 데이터를 각각 리스트 형태로 저장하고 두 데이터를 합치는 과정을 진행한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJqU3vrCFp5D"
      },
      "source": [
        "def read_file(input_path):\n",
        "    \"\"\"Read tsv file, and return words and label as list\"\"\"\n",
        "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        sentences = []\n",
        "        labels = []\n",
        "        for line in f:\n",
        "            split_line = line.strip().split(\"\\t\")\n",
        "            sentences.append(split_line[0])\n",
        "            labels.append(split_line[1])\n",
        "        return sentences, labels\n",
        "    \n",
        "train_sentences, train_labels = read_file(DATA_TRAIN_PATH)\n",
        "test_sentences, test_labels = read_file(DATA_TEST_PATH)\n",
        "ner_sentences = train_sentences + test_sentences\n",
        "ner_labels = train_labels + test_labels\n",
        "\n",
        "ner_dict = {\"sentence\": ner_sentences, \"label\": ner_labels}\n",
        "ner_df = pd.DataFrame(ner_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeE-Ou_ZF2rT"
      },
      "source": [
        "print('전체 ner_data 개수: {}'.format(len(ner_df)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJAnQa-jGAb6"
      },
      "source": [
        "- 개체명 인식 데이터 라벨\n",
        "  - PER\n",
        "  - FLD\n",
        "  - AFW : 인공물\n",
        "  - ORG\n",
        "  - LOC\n",
        "  - CVL\n",
        "  - DAT\n",
        "  - TIM\n",
        "  - NUM\n",
        "  - EVT\n",
        "  - ANM : 동물\n",
        "  - PLT : 식물\n",
        "  - MAT : Material\n",
        "  - TRM : 일반용어(Term)\n",
        "  - O : Outside. 개체명이 아닌 부분"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OHVpeQbwS4T"
      },
      "source": [
        "#### 한국어 개체명 텍스트 분석\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsM7PAfzGrxl"
      },
      "source": [
        "train_set = pd.Series(ner_df[\"sentence\"].tolist())\n",
        "print('유일한 총 문장 수 : {}'.format(len(np.unique(train_set))))\n",
        "print('반복해서 나타나는 문장의 수: {}'.format(np.sum(train_set.value_counts() > 1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPUvJN2qGwOl"
      },
      "source": [
        "train_length = train_set.apply(len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzkDwWIiGxzz"
      },
      "source": [
        "train_set[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF3NPTODG0Nc"
      },
      "source": [
        "print('문장 길이 최대 값: {}'.format(np.max(train_length)))\n",
        "print('문장 길이 평균 값: {:.2f}'.format(np.mean(train_length)))\n",
        "print('문장 길이 표준편차: {:.2f}'.format(np.std(train_length)))\n",
        "print('문장 길이 중간 값: {}'.format(np.median(train_length)))\n",
        "print('문장 길이 제 1 사분위: {}'.format(np.percentile(train_length, 25)))\n",
        "print('문장 길이 제 3 사분위: {}'.format(np.percentile(train_length, 75)))\n",
        "print('문장 길이 99%: {}'.format(np.percentile(train_length, 99)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POzz9ShhHBqo"
      },
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "plt.hist(train_length, bins=200, range=[0,200], facecolor='r', density=True, label='train')\n",
        "plt.title(\"Distribution of character count in sentence\", fontsize=15)\n",
        "plt.legend()\n",
        "plt.xlabel('Number of characters', fontsize=15)\n",
        "plt.ylabel('Probability', fontsize=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuKj1V6GHEA3"
      },
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.boxplot(train_length,\n",
        "             labels=['char counts'],\n",
        "             showmeans=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKEZNt2jHLHz"
      },
      "source": [
        "train_word_counts = train_set.apply(lambda x:len(x.split(' ')))\n",
        "\n",
        "print('문장 단어 개수 최대 값: {}'.format(np.max(train_word_counts)))\n",
        "print('문장 단어 개수 평균 값: {:.2f}'.format(np.mean(train_word_counts)))\n",
        "print('문장 단어 개수 표준편차: {:.2f}'.format(np.std(train_word_counts)))\n",
        "print('문장 단어 개수 중간 값: {}'.format(np.median(train_word_counts)))\n",
        "print('문장 단어 개수 제 1 사분위: {}'.format(np.percentile(train_word_counts, 25)))\n",
        "print('문장 단어 개수 제 3 사분위: {}'.format(np.percentile(train_word_counts, 75)))\n",
        "print('문장 단어 개수 99 퍼센트: {}'.format(np.percentile(train_word_counts, 99)))\n",
        "\n",
        "# 문장 단어 개수 최대 값: 175\n",
        "# 문장 단어 개수 평균 값: 11.81\n",
        "# 문장 단어 개수 표준편차: 7.03\n",
        "# 문장 단어 개수 중간 값: 10.0\n",
        "# 문장 단어 개수 제 1 사분위: 7.0\n",
        "# 문장 단어 개수 제 3 사분위: 15.0\n",
        "# 문장 단어 개수 99 퍼센트: 35.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmq20aW3HS_u"
      },
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.boxplot(train_word_counts,\n",
        "             labels=['counts'],\n",
        "             showmeans=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xHWa4GHwVHE"
      },
      "source": [
        "#### NER 데이터 전처리\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iQjefzKH3lB"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import copy\n",
        "\n",
        "import tensorflow as tf\n",
        "from transformers import *\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "\n",
        "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgaUCJQBH6Ax"
      },
      "source": [
        "# 시각화\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ5cfCIeH8dP"
      },
      "source": [
        "#random seed 고정\n",
        "tf.random.set_seed(1234)\n",
        "np.random.seed(1234)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 3\n",
        "MAX_LEN = 111 # EDA에서 추출된 Max Length\n",
        "DATA_IN_PATH = 'data_in/KOR'\n",
        "DATA_OUT_PATH = \"data_out/KOR\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQqUZLe5ITub"
      },
      "source": [
        "# 데이터 전처리 준비\n",
        "DATA_TRAIN_PATH = os.path.join(DATA_IN_PATH, \"NER\", \"train.tsv\")\n",
        "DATA_LABEL_PATH = os.path.join(DATA_IN_PATH, \"NER\", \"label.txt\")\n",
        "DATA_TEST_PATH = os.path.join(DATA_IN_PATH, \"NER\", \"test.tsv\")\n",
        "\n",
        "def read_file(input_path):\n",
        "    \"\"\"Read tsv file, and return words and label as list\"\"\"\n",
        "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        sentences = []\n",
        "        labels = []\n",
        "        for line in f:\n",
        "            split_line = line.strip().split(\"\\t\")\n",
        "            sentences.append(split_line[0])\n",
        "            labels.append(split_line[1])\n",
        "        return sentences, labels\n",
        "\n",
        "train_sentences, train_labels = read_file(DATA_TRAIN_PATH)\n",
        "\n",
        "train_ner_dict = {\"sentence\": train_sentences, \"label\": train_labels}\n",
        "train_ner_df = pd.DataFrame(train_ner_dict)\n",
        "\n",
        "test_sentences, test_labels = read_file(DATA_TEST_PATH)\n",
        "test_ner_dict = {\"sentence\": test_sentences, \"label\": test_labels}\n",
        "test_ner_df = pd.DataFrame(test_ner_dict)\n",
        "\n",
        "print(\"개체명 인식 학습 데이터 개수: {}\".format(len(train_ner_df)))\n",
        "print(\"개체명 인식 테스트 데이터 개수: {}\".format(len(test_ner_df)))\n",
        "\n",
        "# 개체명 인식 학습 데이터 개수: 81000\n",
        "# 개체명 인식 테스트 데이터 개수: 9000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTvs2FPZIb0k"
      },
      "source": [
        "# Label 불러오기\n",
        "\n",
        "def get_labels(label_path):\n",
        "    return [label.strip() for label in open(os.path.join(label_path), 'r', encoding='utf-8')]\n",
        "\n",
        "ner_labels = get_labels(DATA_LABEL_PATH)\n",
        "\n",
        "print(\"개체명 인식 레이블 개수: {}\".format(len(ner_labels)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v848O49OIfW_"
      },
      "source": [
        "- 다음은 버트 토크나이저를 설정하고 버트에 필요한 각 토큰 값을 설정하는 과정이다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykFr05FsIlOa"
      },
      "source": [
        "# 버트 토크나이저 설정\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", cache_dir='bert_ckpt')\n",
        "\n",
        "pad_token_id = tokenizer.pad_token_id # 0\n",
        "pad_token_label_id = 0\n",
        "cls_token_label_id = 0\n",
        "sep_token_label_id = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPJI6tVkIsHw"
      },
      "source": [
        "# 버트 토크나이저를 통한 학습 데이터를 손쉽게 학습할 수 있도록 bert_tokenizer를 생성한다.\n",
        "def bert_tokenizer(sent, MAX_LEN):\n",
        "    \n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "        text = sent,\n",
        "        truncation=True,\n",
        "        add_special_tokens = True, #'[CLS]'와 '[SEP]' 추가\n",
        "        max_length = MAX_LEN,           # 문장 패딩 및 자르기 진행\n",
        "        pad_to_max_length = True,\n",
        "        return_attention_mask = True   # 어탠션 마스크 생성\n",
        "    )\n",
        "    \n",
        "    input_id = encoded_dict['input_ids']\n",
        "    attention_mask = encoded_dict['attention_mask'] \n",
        "    token_type_id = encoded_dict['token_type_ids']\n",
        "    \n",
        "    return input_id, attention_mask, token_type_id\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpP6OHWRI1XF"
      },
      "source": [
        "- 띄어쓰기 단위의 라베을 토큰 단위로 변환하는 함수를 구성해본다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UByNEl0rI6Eu"
      },
      "source": [
        "def convert_label(words, labels_idx, ner_begin_label, max_seq_len):\n",
        "            \n",
        "    tokens = []\n",
        "    label_ids = []\n",
        "\n",
        "    for word, slot_label in zip(words, labels_idx):\n",
        "\n",
        "        word_tokens = tokenizer.tokenize(word)\n",
        "        if not word_tokens:\n",
        "            word_tokens = [unk_token]\n",
        "        tokens.extend(word_tokens)\n",
        "        \n",
        "        # 슬롯 레이블 값이 Begin이면 I로 추가\n",
        "        if int(slot_label) in ner_begin_label:\n",
        "            label_ids.extend([int(slot_label)] + [int(slot_label) + 1] * (len(word_tokens) - 1))\n",
        "        else:\n",
        "            label_ids.extend([int(slot_label)] * len(word_tokens))\n",
        "  \n",
        "    # [CLS] and [SEP] 설정\n",
        "    special_tokens_count = 2\n",
        "    if len(label_ids) > max_seq_len - special_tokens_count:\n",
        "        label_ids = label_ids[: (max_seq_len - special_tokens_count)]\n",
        "\n",
        "    # [SEP] 토큰 추가\n",
        "    label_ids += [sep_token_label_id]\n",
        "\n",
        "    # [CLS] 토큰 추가\n",
        "    label_ids = [cls_token_label_id] + label_ids\n",
        "    \n",
        "    padding_length = max_seq_len - len(label_ids)\n",
        "    label_ids = label_ids + ([pad_token_label_id] * padding_length)\n",
        "    \n",
        "    return label_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2krgC1IXJbj5"
      },
      "source": [
        "ner_begin_label = [ner_labels.index(begin_label) for begin_label in ner_labels if \"B\" in begin_label]\n",
        "\n",
        "def create_inputs_targets(df):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    token_type_ids = []\n",
        "    label_list = []\n",
        "\n",
        "    # 각 데이터프레임에 있는 문장과 라벨을 띄어쓰기 단위로 만들어 각각 words와 labels 변수에 저장한다.\n",
        "    for i, data in enumerate(df[['sentence', 'label']].values):\n",
        "        sentence, labels = data\n",
        "        words = sentence.split()\n",
        "        labels = labels.split()\n",
        "        labels_idx = []\n",
        "        \n",
        "        # labels_idx에 labels의 값들을 인덱스로 변환한다. \n",
        "        for label in labels:\n",
        "            labels_idx.append(ner_labels.index(label) if label in ner_labels else ner_labels.index(\"UNK\"))\n",
        "\n",
        "        assert len(words) == len(labels_idx)\n",
        "\n",
        "        # 버트에 필요한 것들을 생성하는 과정과 같다.\n",
        "        input_id, attention_mask, token_type_id = bert_tokenizer(sentence, MAX_LEN)\n",
        "\n",
        "        # 앞에서 설명한 띄어쓰기 단위의 라벨을 토큰 단위로 변환하는 함수를 호출해서 라벨 토큰 단위로 변환한다.\n",
        "        convert_label_id = convert_label(words, labels_idx, ner_begin_label, MAX_LEN)\n",
        "\n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "        token_type_ids.append(token_type_id)\n",
        "        label_list.append(convert_label_id)\n",
        "\n",
        "    input_ids = np.array(input_ids, dtype=int)\n",
        "    attention_masks = np.array(attention_masks, dtype=int)\n",
        "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
        "    label_list = np.asarray(label_list, dtype=int) #레이블 토크나이징 리스트\n",
        "    inputs = (input_ids, attention_masks, token_type_ids)\n",
        "    \n",
        "    return inputs, label_list\n",
        "\n",
        "# 넘파이 형태로 치환해서 각 형태에 맞게 반환한다.\n",
        "train_inputs, train_labels = create_inputs_targets(train_ner_df)\n",
        "test_inputs, test_labels = create_inputs_targets(test_ner_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI_JGlsPwcMc"
      },
      "source": [
        "#### NER 모델 학습 및 테스트\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdXi7oKsKNuN"
      },
      "source": [
        "class TFBertNERClassifier(tf.keras.Model):\n",
        "    def __init__(self, model_name, dir_path, num_class):\n",
        "        super(TFBertNERClassifier, self).__init__()\n",
        "\n",
        "        self.bert = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\n",
        "        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n",
        "        self.classifier = tf.keras.layers.Dense(num_class, \n",
        "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range),\n",
        "                                                name=\"ner_classifier\")\n",
        "\n",
        "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
        "\n",
        "        #outputs 값: # sequence_output, pooled_output, (hidden_states), (attentions)\n",
        "        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        sequence_output = outputs[0]\n",
        "                \n",
        "        sequence_output = self.dropout(sequence_output, training=training)\n",
        "        logits = self.classifier(sequence_output)\n",
        "        \n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ct7efb_KWY3"
      },
      "source": [
        "- 현재 개체명 인식 모델은 문장을 토큰화해서 각각 한개의 토큰에 대한 라벨이 있어 각각을 전부 분류해야 하는 시퀀스 라벨 문제가 생겼다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHPet8p5KWDp"
      },
      "source": [
        "ner_model = TFBertNERClassifier(model_name='bert-base-multilingual-cased',\n",
        "                                  dir_path='bert_ckpt',\n",
        "                                  num_class=len(ner_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGpRtICeKloE"
      },
      "source": [
        "- 이번에는 손실 값을 확인해보자. 개체명 분류는 시퀀스 분류이기 때문에 기존과 다르게 직접 손실 값 함수를 구현하여야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEPMBUW6KlOB"
      },
      "source": [
        "def compute_loss(labels, logits):\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
        "    )\n",
        "\n",
        "    # 0의 레이블 값은 손실 값을 계산할 때 제외\n",
        "    active_loss = tf.reshape(labels, (-1,)) != 0\n",
        "        \n",
        "    reduced_logits = tf.boolean_mask(tf.reshape(logits, (-1, shape_list(logits)[2])), active_loss)\n",
        "        \n",
        "    labels = tf.boolean_mask(tf.reshape(labels, (-1,)), active_loss)\n",
        "    \n",
        "    return loss_fn(labels, reduced_logits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_lrFDnHK_Us"
      },
      "source": [
        "- 이번에는 accuracy를 사용하기 힘들기 때문에 F1 score를 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80S73t8sLDX6"
      },
      "source": [
        "class F1Metrics(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, x_eval, y_eval):\n",
        "        self.x_eval = x_eval\n",
        "        self.y_eval = y_eval\n",
        "\n",
        "    def compute_f1_pre_rec(self, labels, preds):\n",
        "\n",
        "        return {\n",
        "            \"precision\": precision_score(labels, preds, suffix=True),\n",
        "            \"recall\": recall_score(labels, preds, suffix=True),\n",
        "            \"f1\": f1_score(labels, preds, suffix=True)\n",
        "        }\n",
        "\n",
        "\n",
        "    def show_report(self, labels, preds):\n",
        "        return classification_report(labels, preds, suffix=True)\n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "        results = {}\n",
        "        \n",
        "        pred = self.model.predict(self.x_eval)\n",
        "        label = self.y_eval\n",
        "        pred_argmax = np.argmax(pred, axis = 2)\n",
        "\n",
        "        slot_label_map = {i: label for i, label in enumerate(ner_labels)}\n",
        "\n",
        "        out_label_list = [[] for _ in range(label.shape[0])]\n",
        "        preds_list = [[] for _ in range(label.shape[0])]\n",
        "\n",
        "        for i in range(label.shape[0]):\n",
        "            for j in range(label.shape[1]):\n",
        "                if label[i, j] != 0:\n",
        "                    out_label_list[i].append(slot_label_map[label[i][j]])\n",
        "                    preds_list[i].append(slot_label_map[pred_argmax[i][j]])\n",
        "                    \n",
        "        result = self.compute_f1_pre_rec(out_label_list, preds_list)\n",
        "        results.update(result)\n",
        "\n",
        "        print(\"********\")\n",
        "        print(\"F1 Score\")\n",
        "        for key in sorted(results.keys()):\n",
        "            print(\"{}, {:.4f}\".format(key, results[key]))\n",
        "        print(\"\\n\" + self.show_report(out_label_list, preds_list))\n",
        "        print(\"********\")\n",
        "\n",
        "f1_score_callback = F1Metrics(test_inputs, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pzXYdyCLRLA"
      },
      "source": [
        "- 이후는 직접 제작한 손실값 함수와 옵티마이저 및 학습 메트릭을 설정한 후 컴파일을 진행한다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x44iV-d4LV5Z"
      },
      "source": [
        "# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule\n",
        "optimizer = tf.keras.optimizers.Adam(3e-5)\n",
        "# ner_model.compile(optimizer=optimizer, loss=compute_loss, run_eagerly=True)\n",
        "ner_model.compile(optimizer=optimizer, loss=compute_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELpCLc7oLZwK"
      },
      "source": [
        "model_name = \"tf2_bert_ner\"\n",
        "\n",
        "checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create path if exists\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
        "else:\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
        "    \n",
        "cp_callback = ModelCheckpoint(\n",
        "    checkpoint_path, verbose=1, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "history = ner_model.fit(train_inputs, train_labels, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
        "                        callbacks=[cp_callback, f1_score_callback])\n",
        "\n",
        "print(history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcBErYDtLfak"
      },
      "source": [
        "- 최종적으로 학습을 완료한 후 에폭마다 학습 데이터의 손실 값을 구한 결과는 다음과 같다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjitzwVyLdle"
      },
      "source": [
        "plot_graphs(history, 'loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ01Sll7weXA"
      },
      "source": [
        "### 버트를 활용한 한국어 텍스트 유사도 모델\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ4FEzwjwkPe"
      },
      "source": [
        "#### STS 데이터 분석\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiAiKvy-wrRY"
      },
      "source": [
        "### 버트를 활용한 한국어 기계 독해 모델\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJtgJlq1wtfw"
      },
      "source": [
        "#### KorQuAD 1.0 데이터분석\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vouZxRAQw1D6"
      },
      "source": [
        "#### KorQuAD 1.0 데이터 전처리\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuKRtPNSw6Z3"
      },
      "source": [
        "#### KorQuAD 1.0 학습 및 테스트\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m4c2Zybw_RV"
      },
      "source": [
        "## 03 GPT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK_Xn46mxAzA"
      },
      "source": [
        "### GPT1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tujl6fsAxCoP"
      },
      "source": [
        "### GPT2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw5L-i9QxH9v"
      },
      "source": [
        "## 04 GPT2를 활용한 미세 조정 학습\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb4htVuWxLxH"
      },
      "source": [
        "### GPT2를 활용한 한국어 언어 생성 모델 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQCegVm8xQWe"
      },
      "source": [
        "#### 사전 학습 모델 문장 생성\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMpJUPChxUdf"
      },
      "source": [
        "#### 소설 텍스트 데이터 전처리하기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTi6K0JAxXVf"
      },
      "source": [
        "#### 소설 텍스트 미세 조정 모델 학습\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM2FsCZcxbPa"
      },
      "source": [
        "#### 미세 조정 학습 결과 확인\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jofxxLJxc2Y"
      },
      "source": [
        "### GPT2를 활용한 한국어 텍스트 분류 모델\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxfPeW76xgYf"
      },
      "source": [
        "#### 네이버 영화 리뷰 데이터 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrDaepbUxnRp"
      },
      "source": [
        "#### 네이버 영화 리뷰 모델 학습\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S_tGghuxqcQ"
      },
      "source": [
        "#### 네이버 영화 리뷰 모델 테스트\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFQpx4tAxtNl"
      },
      "source": [
        "### GPT2를 활용한 한국어 자연어 추론 모델\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgcoDnC0xwyR"
      },
      "source": [
        "#### KorNLI 데이터 전처리\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUNSPvG_x10q"
      },
      "source": [
        "#### KorNLI 모델 학습\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeK4wMPrx49X"
      },
      "source": [
        "#### KorNLI 모델 테스트\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-f7uh9ex8FA"
      },
      "source": [
        "### GPT2를 활용한 한국어 텍스트 유사도 모델\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjcXoE_8x-oT"
      },
      "source": [
        "#### KorSTS 데이터 전처리\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGfP_i_byAPB"
      },
      "source": [
        "#### KorSTS 모델 학습\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XQKfvAiyGJF"
      },
      "source": [
        "#### KorSTS 모델 테스"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn0-RPgAyJAO"
      },
      "source": [
        "## 05 정리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlbxjSG4vWlT"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2NM5ER8uJg-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}