{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chapter06.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMQ5OwHlLjyUq5noRLcJMs0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waltechel/tensorflow-ml-nlp-tf2/blob/master/6.CHATBOT/chapter06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x76D93SbNJm"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgs7Yfzdb9Ji"
      },
      "source": [
        "## 00 사전 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF0mssBSbwXG"
      },
      "source": [
        "- 사전 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmKAKpdlakGz"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from konlpy.tag import Okt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsxiyCpgmUch"
      },
      "source": [
        "- 정규식에서 사용할 필터와 특별한 토큰인 PAD, SOS, END, UNK\n",
        "  - PAD : 어떤 의미도 없는 패딩토큰이다.\n",
        "  - SOS : 시작 토큰\n",
        "  - END : 종료 토큰\n",
        "  - UNK : 사전에 없는 단어"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWrBAMSabIvR"
      },
      "source": [
        "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "PAD = \"<PAD>\" \n",
        "STD = \"<SOS>\"\n",
        "END = \"<END>\"\n",
        "UNK = \"<UNK>\"\n",
        "\n",
        "PAD_INDEX = 0\n",
        "STD_INDEX = 1\n",
        "END_INDEX = 2\n",
        "UNK_INDEX = 3\n",
        "\n",
        "MARKER = [PAD, STD, END, UNK]\n",
        "CHANGE_FILTER = re.compile(FILTERS)\n",
        "\n",
        "MAX_SEQUENCE = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Thrc5lxvmpKe"
      },
      "source": [
        "- 판다스를 통해서 데이터를 불러오는 load_data function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C28hq50XbcCT"
      },
      "source": [
        "def load_data(path):\n",
        "    # 판다스를 통해서 데이터를 불러온다.\n",
        "    data_df = pd.read_csv(path, header=0)\n",
        "    # 질문과 답변 열을 가져와 question과 answer에 넣는다.\n",
        "    question, answer = list(data_df['Q']), list(data_df['A'])\n",
        "\n",
        "    return question, answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EcabKlhmvEp"
      },
      "source": [
        "- 데이터를 전처리한 후 단어 리스트로 만드는 data_tokenizer function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYpXqZ6bbeG7"
      },
      "source": [
        "def data_tokenizer(data):\n",
        "    # 토크나이징 해서 담을 배열 생성\n",
        "    words = []\n",
        "    for sentence in data:\n",
        "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "        # 위 필터와 같은 값들을 정규화 표현식을\n",
        "        # 통해서 모두 \"\" 으로 변환 해주는 부분이다.\n",
        "        sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n",
        "        for word in sentence.split():\n",
        "            words.append(word)\n",
        "    # 토그나이징과 정규표현식을 통해 만들어진\n",
        "    # 값들을 넘겨 준다.\n",
        "    return [word for word in words if word]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f16E2W5m8FR"
      },
      "source": [
        "- 한글 텍스트를 토크나이징하기 위해 형태소로 분리하는 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1QkKhzvbfjW"
      },
      "source": [
        "def prepro_like_morphlized(data):\n",
        "    morph_analyzer = Okt()\n",
        "    result_data = list()\n",
        "    for seq in tqdm(data):\n",
        "        morphlized_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ', '')))\n",
        "        result_data.append(morphlized_seq)\n",
        "\n",
        "    return result_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KuxxSC8ohgY"
      },
      "source": [
        "- 단어 사전을 불러오는 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATpQnIIRbhqo"
      },
      "source": [
        "def load_vocabulary(path, vocab_path, tokenize_as_morph=False):\n",
        "    # 사전을 담을 배열 준비한다.\n",
        "    vocabulary_list = []\n",
        "    # 사전을 구성한 후 파일로 저장 진행한다.\n",
        "    # 그 파일의 존재 유무를 확인한다.\n",
        "    if not os.path.exists(vocab_path):\n",
        "        # 이미 생성된 사전 파일이 존재하지 않으므로\n",
        "        # 데이터를 가지고 만들어야 한다.\n",
        "        # 그래서 데이터가 존재 하면 사전을 만들기 위해서\n",
        "        # 데이터 파일의 존재 유무를 확인한다.\n",
        "        if (os.path.exists(path)):\n",
        "            # 데이터가 존재하니 판다스를 통해서 데이터를 불러오자\n",
        "            data_df = pd.read_csv(path, encoding='utf-8')\n",
        "            # 판다스의 데이터 프레임을 통해서 질문과 답에 대한 열을 가져 온다.\n",
        "            question, answer = list(data_df['Q']), list(data_df['A'])\n",
        "            if tokenize_as_morph:  # 형태소에 따른 토크나이저 처리\n",
        "                question = prepro_like_morphlized(question)\n",
        "                answer = prepro_like_morphlized(answer)\n",
        "            data = []\n",
        "            # 질문과 답변을 extend을\n",
        "            # 통해서 구조가 없는 배열로 만든다.\n",
        "            data.extend(question)\n",
        "            data.extend(answer)\n",
        "            # 토크나이저 처리 하는 부분이다.\n",
        "            words = data_tokenizer(data)\n",
        "            # 공통적인 단어에 대해서는 모두 필요 없으므로 집합으로 만들어 주기 위해서\n",
        "            # set해주고 이것들을 리스트로 만들어 준다.\n",
        "            words = list(set(words))\n",
        "            # 데이터 없는 내용중에 MARKER를 사전에\n",
        "            # 추가 하기 위해서 아래와 같이 처리 한다.\n",
        "            # 아래는 MARKER 값이며 리스트의 첫번째 부터\n",
        "            # 순서대로 넣기 위해서 인덱스 0에 추가한다.\n",
        "            # PAD = \"<PADDING>\"\n",
        "            # STD = \"<START>\"\n",
        "            # END = \"<END>\"\n",
        "            # UNK = \"<UNKNWON>\"\n",
        "            words[:0] = MARKER\n",
        "        # 사전을 리스트로 만들었으니 이 내용을\n",
        "        # 사전 파일을 만들어 넣는다.\n",
        "        with open(vocab_path, 'w', encoding='utf-8') as vocabulary_file:\n",
        "            for word in words:\n",
        "                vocabulary_file.write(word + '\\n')\n",
        "\n",
        "    # 사전 파일이 존재하면 여기에서\n",
        "    # 그 파일을 불러서 배열에 넣어 준다.\n",
        "    with open(vocab_path, 'r', encoding='utf-8') as vocabulary_file:\n",
        "        for line in vocabulary_file:\n",
        "            vocabulary_list.append(line.strip())\n",
        "\n",
        "    # 배열에 내용을 키와 값이 있는\n",
        "    # 딕셔너리 구조로 만든다.\n",
        "    char2idx, idx2char = make_vocabulary(vocabulary_list)\n",
        "    # 두가지 형태의 키와 값이 있는 형태를 리턴한다.\n",
        "    # (예) 단어: 인덱스 , 인덱스: 단어)\n",
        "    return char2idx, idx2char, len(char2idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0bwVghMolOt"
      },
      "source": [
        "- 단어 사전을 만드는 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LylS9jxCbkmT"
      },
      "source": [
        "def make_vocabulary(vocabulary_list):\n",
        "    # 리스트를 키가 단어이고 값이 인덱스인\n",
        "    # 딕셔너리를 만든다.\n",
        "    char2idx = {char: idx for idx, char in enumerate(vocabulary_list)}\n",
        "    # 리스트를 키가 인덱스이고 값이 단어인\n",
        "    # 딕셔너리를 만든다.\n",
        "    idx2char = {idx: char for idx, char in enumerate(vocabulary_list)}\n",
        "    # 두개의 딕셔너리를 넘겨 준다.\n",
        "    return char2idx, idx2char"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsOs4UpyotBA"
      },
      "source": [
        "- 인코더에 적용될 입력값을 만드는 전처리 함수를 확인해보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuqXHjG_bmr_"
      },
      "source": [
        "def enc_processing(value, dictionary, tokenize_as_morph=False):\n",
        "    # 인덱스 값들을 가지고 있는 배열이다.(누적된다.)\n",
        "    sequences_input_index = []\n",
        "    # 하나의 인코딩 되는 문장의 길이를 가지고 있다.(누적된다.)\n",
        "    sequences_length = []\n",
        "    # 형태소 토크나이징 사용 유무\n",
        "    if tokenize_as_morph:\n",
        "        value = prepro_like_morphlized(value)\n",
        "\n",
        "    # 한줄씩 불어온다.\n",
        "    for sequence in value:\n",
        "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "        # 정규화를 사용하여 필터에 들어 있는 값들을 \"\" 으로 치환 한다.\n",
        "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
        "        # 하나의 문장을 인코딩 할때 가지고 있기 위한 배열이다.\n",
        "        sequence_index = []\n",
        "        # 문장을 스페이스 단위로 자른다.\n",
        "        for word in sequence.split():\n",
        "            # 잘려진 단어들이 딕셔너리에 존재 하는지 보고\n",
        "            # 그 값을 가져와 sequence_index에 추가한다.\n",
        "            if dictionary.get(word) is not None:\n",
        "                sequence_index.extend([dictionary[word]])\n",
        "            # 잘려진 단어가 딕셔너리에 존재 하지 않는\n",
        "            # 경우 이므로 UNK(2)를 넣어 준다.\n",
        "            else:\n",
        "                sequence_index.extend([dictionary[UNK]])\n",
        "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
        "        if len(sequence_index) > MAX_SEQUENCE:\n",
        "            sequence_index = sequence_index[:MAX_SEQUENCE]\n",
        "        # 하나의 문장에 길이를 넣어주고 있다.\n",
        "        sequences_length.append(len(sequence_index))\n",
        "        # max_sequence_length보다 문장 길이가\n",
        "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
        "        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
        "        # 인덱스화 되어 있는 값을\n",
        "        # sequences_input_index에 넣어 준다.\n",
        "        sequences_input_index.append(sequence_index)\n",
        "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n",
        "    # 이유는 텐서플로우 dataset에 넣어 주기 위한\n",
        "    # 사전 작업이다.\n",
        "    # 넘파이 배열에 인덱스화된 배열과\n",
        "    # 그 길이를 넘겨준다.\n",
        "    return np.asarray(sequences_input_index), sequences_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m4AmatMpBmF"
      },
      "source": [
        "- 디코더의 입력값을 만드는 함수를 만든다.\n",
        "  - 디코딩 입력의 처음에는 START가 와야 하므로 그 값을 넣어 주고 시작한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnIR8Udzbo3j"
      },
      "source": [
        "def dec_output_processing(value, dictionary, tokenize_as_morph=False):\n",
        "    # 인덱스 값들을 가지고 있는 배열이다.(누적된다)\n",
        "    sequences_output_index = []\n",
        "    # 하나의 디코딩 입력 되는 문장의 길이를 가지고 있다.(누적된다)\n",
        "    sequences_length = []\n",
        "    # 형태소 토크나이징 사용 유무\n",
        "    if tokenize_as_morph:\n",
        "        value = prepro_like_morphlized(value)\n",
        "    # 한줄씩 불어온다.\n",
        "    for sequence in value:\n",
        "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "        # 정규화를 사용하여 필터에 들어 있는 값들을 \"\" 으로 치환 한다.\n",
        "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
        "        # 하나의 문장을 디코딩 할때 가지고 있기 위한 배열이다.\n",
        "        sequence_index = []\n",
        "        # 디코딩 입력의 처음에는 START가 와야 하므로 그 값을 넣어 주고 시작한다.\n",
        "        # 문장에서 스페이스 단위별로 단어를 가져와서 딕셔너리의 값인 인덱스를 넣어 준다.\n",
        "        sequence_index = [dictionary[STD]] + [dictionary[word] if word in dictionary else dictionary[UNK] for word in sequence.split()]\n",
        "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
        "        if len(sequence_index) > MAX_SEQUENCE:\n",
        "            sequence_index = sequence_index[:MAX_SEQUENCE]\n",
        "        # 하나의 문장에 길이를 넣어주고 있다.\n",
        "        sequences_length.append(len(sequence_index))\n",
        "        # max_sequence_length보다 문장 길이가 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
        "        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
        "        # 인덱스화 되어 있는 값을 sequences_output_index 넣어 준다.\n",
        "        sequences_output_index.append(sequence_index)\n",
        "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n",
        "    # 이유는 텐서플로우 dataset에 넣어 주기 위한 사전 작업이다.\n",
        "    # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n",
        "    return np.asarray(sequences_output_index), sequences_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IZ6w_AkpHhj"
      },
      "source": [
        "- 디코더의 타겟값을 만드는 전처리 함수\n",
        "  - 문장이 시작하는 부분에 시작 토큰을 넣지 않고 끝나는 부분에 종료 토큰을 넣는다는 점이 다르다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIASVNKvbqdk"
      },
      "source": [
        "def dec_target_processing(value, dictionary, tokenize_as_morph=False):\n",
        "    # 인덱스 값들을 가지고 있는 배열이다.(누적된다)\n",
        "    sequences_target_index = []\n",
        "    # 형태소 토크나이징 사용 유무\n",
        "    if tokenize_as_morph:\n",
        "        value = prepro_like_morphlized(value)\n",
        "    # 한줄씩 불어온다.\n",
        "    for sequence in value:\n",
        "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "        # 정규화를 사용하여 필터에 들어 있는 값들을 \"\" 으로 치환 한다.\n",
        "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
        "        # 문장에서 스페이스 단위별로 단어를 가져와서\n",
        "        # 딕셔너리의 값인 인덱스를 넣어 준다.\n",
        "        # 디코딩 출력의 마지막에 END를 넣어 준다.\n",
        "        sequence_index = [dictionary[word] if word in dictionary else dictionary[UNK] for word in sequence.split()]\n",
        "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
        "        # 그리고 END 토큰을 넣어 준다\n",
        "        if len(sequence_index) >= MAX_SEQUENCE:\n",
        "            sequence_index = sequence_index[:MAX_SEQUENCE - 1] + [dictionary[END]]\n",
        "        else:\n",
        "          # 문장이 시작하는 부분에 시작 토큰을 넣지 않고 끝나는 부분에 종료 토큰을 넣는다는 점이 다르다.\n",
        "            sequence_index += [dictionary[END]]\n",
        "        # max_sequence_length보다 문장 길이가\n",
        "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
        "        sequence_index += (MAX_SEQUENCE - len(sequence_index)) * [dictionary[PAD]]\n",
        "        # 인덱스화 되어 있는 값을\n",
        "        # sequences_target_index에 넣어 준다.\n",
        "        sequences_target_index.append(sequence_index)\n",
        "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n",
        "    # 이유는 텐서플로우 dataset에 넣어 주기 위한 사전 작업이다.\n",
        "    # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n",
        "    return np.asarray(sequences_target_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RzZ8RNKhOwN"
      },
      "source": [
        "## 01 데이터 소개"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1s5GVeChQCj"
      },
      "source": [
        "- 이 데이터는 총 11,876개의 데이터로 구성돼 있고, 각 데이터는 질문과 그에대한 대답, 그리고 주제에 대한 라벨값을 가지고 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zM4xWqhb4wL"
      },
      "source": [
        "## 02 EDA(데이터 분석)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oPh0svRbsoZ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "from functools import reduce\n",
        "from wordcloud import WordCloud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hobpkw-Qhmoe"
      },
      "source": [
        "- 먼저 데이터 분석을 위해 데이터를 불러온다.\n",
        "- 판다스 라이브러리를 사용해 데이터프레임 형태로 데이터를 불러온다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CANJKxGcBHg"
      },
      "source": [
        "DATA_IN_PATH = './data_in/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1bg9_rqhtAY"
      },
      "source": [
        "- 깃허브에 들어 있는 파일을 업로드한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeXZEPs1cCM5"
      },
      "source": [
        "data = pd.read_csv(DATA_IN_PATH + 'ChatBotData.csv', encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_m-qBJrcD_0"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iNHL2Txh6L6"
      },
      "source": [
        "- 데이터 구조를 확인해보기 위해 head 함수를 사용해 데이터의 일부만 출력해본다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpWxS8HUcWAf"
      },
      "source": [
        "print(data.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxPMdYXViD2d"
      },
      "source": [
        "- 질문과 답변 모두에 대해 길이를 분석하기 위해 두 데이터를 하나의 리스트로 만든다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJLY4rlucXt0"
      },
      "source": [
        "sentences = list(data['Q']) + list(data['A'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2qH5DaxcZRW"
      },
      "source": [
        "# 띄어쓰기 기준으로 문장을 나눈다.\n",
        "tokenized_sentences = [s.split() for s in sentences]\n",
        "# 이 값의 길이를 측정해서 어절의 길이를 측정하고\n",
        "sent_len_by_token = [len(t) for t in tokenized_sentences]\n",
        "# 이 값을 다시 붙여서 음절의 길이로 사용한다.\n",
        "sent_len_by_eumjeol = [len(s.replace(' ', '')) for s in sentences]\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "# Okt를 활용해서 형태소의 길이를 측정한다.\n",
        "morph_tokenized_sentences = [okt.morphs(s.replace(' ', '')) for s in sentences]\n",
        "sent_len_by_morph = [len(t) for t in morph_tokenized_sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEurX8_1inti"
      },
      "source": [
        "- 빨간색 히스토그램은 어절 단위, 초록색은 형태소 단위, 파란색은 음절 단위를 나타낸 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTPbZJuycaua"
      },
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "plt.hist(sent_len_by_token, bins=50, range=[0,50], alpha=0.5, color= 'r', label='eojeol')\n",
        "plt.hist(sent_len_by_morph, bins=50, range=[0,50], alpha=0.5, color='g', label='morph')\n",
        "plt.hist(sent_len_by_eumjeol, bins=50, range=[0,50], alpha=0.5, color='b', label='eumjeol')\n",
        "plt.title('Sentence Length Histogram')\n",
        "plt.xlabel('Sentence Length')\n",
        "plt.ylabel('Number of Sentences')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVRqLrSijCBq"
      },
      "source": [
        "- 중간에 yscale 함수를사용했다. y 값의 스케일을 조정함으로써 차이가 큰 데이터에 대해서도 비교할 수 있게 만든다\n",
        "- 이렇게 했을 때 데이터가 바뀌는 것은 아니고 로그 스케일로 비교한 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osaLrcqQccL6"
      },
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "plt.hist(sent_len_by_token, bins=50, range=[0,50], alpha=0.5, color= 'r', label='eojeol')\n",
        "plt.hist(sent_len_by_morph, bins=50, range=[0,50], alpha=0.5, color='g', label='morph')\n",
        "plt.hist(sent_len_by_eumjeol, bins=50, range=[0,50], alpha=0.5, color='b', label='eumjeol')\n",
        "plt.yscale('log')\n",
        "plt.title('Sentence Length Histogram by Eojeol Token')\n",
        "plt.xlabel('Sentence Length')\n",
        "plt.ylabel('Number of Sentences')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz7c15GWjP5Y"
      },
      "source": [
        "- 어절, 형태소, 음절 단위의 통곗값을 비교해본다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClY42qL4cdlZ"
      },
      "source": [
        "print('어절 최대길이: {}'.format(np.max(sent_len_by_token)))\n",
        "print('어절 최소길이: {}'.format(np.min(sent_len_by_token)))\n",
        "print('어절 평균길이: {:.2f}'.format(np.mean(sent_len_by_token)))\n",
        "print('어절 길이 표준편차: {:.2f}'.format(np.std(sent_len_by_token)))\n",
        "print('어절 중간길이: {}'.format(np.median(sent_len_by_token)))\n",
        "print('제 1 사분위 길이: {}'.format(np.percentile(sent_len_by_token, 25)))\n",
        "print('제 3 사분위 길이: {}'.format(np.percentile(sent_len_by_token, 75)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3luswYlice-r"
      },
      "source": [
        "print('형태소 최대길이: {}'.format(np.max(sent_len_by_morph)))\n",
        "print('형태소 최소길이: {}'.format(np.min(sent_len_by_morph)))\n",
        "print('형태소 평균길이: {:.2f}'.format(np.mean(sent_len_by_morph)))\n",
        "print('형태소 길이 표준편차: {:.2f}'.format(np.std(sent_len_by_morph)))\n",
        "print('형태소 중간길이: {}'.format(np.median(sent_len_by_morph)))\n",
        "print('형태소 1/4 퍼센타일 길이: {}'.format(np.percentile(sent_len_by_morph, 25)))\n",
        "print('형태소 3/4 퍼센타일 길이: {}'.format(np.percentile(sent_len_by_morph, 75)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb6NBDhzcgWm"
      },
      "source": [
        "print('음절 최대길이: {}'.format(np.max(sent_len_by_eumjeol)))\n",
        "print('음절 최소길이: {}'.format(np.min(sent_len_by_eumjeol)))\n",
        "print('음절 평균길이: {:.2f}'.format(np.mean(sent_len_by_eumjeol)))\n",
        "print('음절 길이 표준편차: {:.2f}'.format(np.std(sent_len_by_eumjeol)))\n",
        "print('음절 중간길이: {}'.format(np.median(sent_len_by_eumjeol)))\n",
        "print('음절 1/4 퍼센타일 길이: {}'.format(np.percentile(sent_len_by_eumjeol, 25)))\n",
        "print('음절 3/4 퍼센타일 길이: {}'.format(np.percentile(sent_len_by_eumjeol, 75)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9TSuKq1jVO9"
      },
      "source": [
        "- 박스 플롯을 그려서 긴 꼬리 분포를 확인해본다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22sTWRt7chay"
      },
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "plt.boxplot([sent_len_by_token, sent_len_by_morph, sent_len_by_eumjeol],\n",
        "            labels=['Eojeol', 'Morph', 'Eumjeol'], \n",
        "            showmeans=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khF1hfJRjiEF"
      },
      "source": [
        "- 이제부터는 질문과 답변만 살펴보되, 형태소 단위로만 비교하도록 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRG7AwDgcioF"
      },
      "source": [
        "query_sentences = list(data['Q'])\n",
        "answer_sentences = list(data['A'])\n",
        "\n",
        "query_morph_tokenized_sentences = [okt.morphs(s.replace(' ', '')) for s in query_sentences]\n",
        "query_sent_len_by_morph = [len(t) for t in query_morph_tokenized_sentences]\n",
        "\n",
        "answer_morph_tokenized_sentences = [okt.morphs(s.replace(' ', '')) for s in answer_sentences]\n",
        "answer_sent_len_by_morph = [len(t) for t in answer_morph_tokenized_sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cHxRRwdjnHS"
      },
      "source": [
        "- 형태소로 나눈 길이를 히스토그램으로 살펴본다. 질문과 답변에 대한 길이를 한번에 히스토그램으로 그린다.\n",
        "  - 질문의 길이가 대답의 길이보다는 짧다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wSg3WXtcjvU"
      },
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "plt.hist(query_sent_len_by_morph, bins=50, range=[0,50], color='g', label='Query')\n",
        "plt.hist(answer_sent_len_by_morph, bins=50, range=[0,50], color='r', alpha=0.5, label='Answer')\n",
        "plt.legend()\n",
        "plt.title('Query Length Histogram by Morph Token')\n",
        "plt.xlabel('Query Length')\n",
        "plt.ylabel('Number of Queries')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiQM5NYij0V-"
      },
      "source": [
        "- yscale을 먹여준다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUGfmufOck1R"
      },
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "plt.hist(query_sent_len_by_morph, bins=50, range=[0,50], color='g', label='Query')\n",
        "plt.hist(answer_sent_len_by_morph, bins=50, range=[0,50], color='r', alpha=0.5, label='Answer')\n",
        "plt.legend()\n",
        "plt.yscale('log', nonposy='clip')\n",
        "plt.title('Query Length Log Histogram by Morph Token')\n",
        "plt.xlabel('Query Length')\n",
        "plt.ylabel('Number of Queries')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zke1WEATj45x"
      },
      "source": [
        "- 질문과 대답에 대한 형태소를 분석한다.\n",
        "- 질문에 대해 형태소 분석한 것의 통곗값을 확인한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxGFX2vecmGx"
      },
      "source": [
        "print('형태소 최대길이: {}'.format(np.max(query_sent_len_by_morph)))\n",
        "print('형태소 최소길이: {}'.format(np.min(query_sent_len_by_morph)))\n",
        "print('형태소 평균길이: {:.2f}'.format(np.mean(query_sent_len_by_morph)))\n",
        "print('형태소 길이 표준편차: {:.2f}'.format(np.std(query_sent_len_by_morph)))\n",
        "print('형태소 중간길이: {}'.format(np.median(query_sent_len_by_morph)))\n",
        "print('형태소 1/4 퍼센타일 길이: {}'.format(np.percentile(query_sent_len_by_morph, 25)))\n",
        "print('형태소 3/4 퍼센타일 길이: {}'.format(np.percentile(query_sent_len_by_morph, 75)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XHuh6prkBvy"
      },
      "source": [
        "- 대답에 대해 형태소 분석한 것의 통곗값을 확인한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-IfKmk6comG"
      },
      "source": [
        "print('형태소 최대길이: {}'.format(np.max(answer_sent_len_by_morph)))\n",
        "print('형태소 최소길이: {}'.format(np.min(answer_sent_len_by_morph)))\n",
        "print('형태소 평균길이: {:.2f}'.format(np.mean(answer_sent_len_by_morph)))\n",
        "print('형태소 길이 표준편차: {:.2f}'.format(np.std(answer_sent_len_by_morph)))\n",
        "print('형태소 중간길이: {}'.format(np.median(answer_sent_len_by_morph)))\n",
        "print('형태소 1/4 퍼센타일 길이: {}'.format(np.percentile(answer_sent_len_by_morph, 25)))\n",
        "print('형태소 3/4 퍼센타일 길이: {}'.format(np.percentile(answer_sent_len_by_morph, 75)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amtCYkkvkQEM"
      },
      "source": [
        "- 두 데이터를 박스 플롯으로 그려본다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csHd5DCikZUP"
      },
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "plt.boxplot([query_sent_len_by_morph, answer_sent_len_by_morph],\n",
        "            labels=['Query', 'Answer'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU6NzZg0kqr6"
      },
      "source": [
        "- 사례를 확인해본다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB4ZNKRScp3E"
      },
      "source": [
        "okt.pos('오늘밤은유난히덥구나')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XccM7UTbksYC"
      },
      "source": [
        "- 이중에서 tag가 명사Noun, 동사Verb, 형용사Adjective만 취하도록 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO8hJGBncq8V"
      },
      "source": [
        "query_NVA_token_sentences = list()\n",
        "answer_NVA_token_sentences = list()\n",
        "\n",
        "for s in query_sentences:\n",
        "    for token, tag in okt.pos(s.replace(' ', '')):\n",
        "        if tag == 'Noun' or tag == 'Verb' or tag == 'Adjective':\n",
        "            query_NVA_token_sentences.append(token)\n",
        "\n",
        "for s in answer_sentences:\n",
        "    temp_token_bucket = list()\n",
        "    for token, tag in okt.pos(s.replace(' ', '')):\n",
        "        if tag == 'Noun' or tag == 'Verb' or tag == 'Adjective':\n",
        "            answer_NVA_token_sentences.append(token)\n",
        "            \n",
        "query_NVA_token_sentences = ' '.join(query_NVA_token_sentences)\n",
        "answer_NVA_token_sentences = ' '.join(answer_NVA_token_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRVTuWs8k3c1"
      },
      "source": [
        "- 질문에 대한 워드클라우드\n",
        "  - 사랑, 연애, 친구 등등의 말이 많이 나오는 것으로 보아 연애에 대한 적중이 높은 것 같다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qam9AE4PcsSh"
      },
      "source": [
        "query_wordcloud = WordCloud(font_path= DATA_IN_PATH + 'NanumGothic.ttf').generate(query_NVA_token_sentences)\n",
        "\n",
        "plt.imshow(query_wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byGlA_s0lK1e"
      },
      "source": [
        "- 사람, 사랑, 마음, 생각, 해보라는 뜻의 답이 많은 것으로 보아 적극적인 답변이 기대된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyNv_JLDctOn"
      },
      "source": [
        "query_wordcloud = WordCloud(font_path= DATA_IN_PATH + 'NanumGothic.ttf').generate(answer_NVA_token_sentences)\n",
        "\n",
        "plt.imshow(query_wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSGz8-Z0lTn3"
      },
      "source": [
        "## 03 seq2seq 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFt4GWVTlYCx"
      },
      "source": [
        "- 시퀀스 투 시퀀스\n",
        "  - 시퀀스 형태의 입력값을 시퀀스 형태의 출력으로 만들 수 있게 만든 모델\n",
        "  - 기계 번역, 텍스트 요약, 이미지 캡션, 대화 모델 등 다양한 분야에서 활용된다.\n",
        "  - 인코더 부분에서 입력값을 받아 입력값의 정보를 담은 벡터를 만들어낸다\n",
        "  - 디코더에서는 이 벡터를 활용해 재귀적으로 출력값을 만들어낸다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdW8Lupvl8W2"
      },
      "source": [
        "- 재현 신경망의 경우 고정된 문장 길이를 정해야 한다\n",
        "  - 빈 단어는 패딩으로 채워 넣으므로 실제 문장 길이가 고정되는 것은 아니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdI930kxdUz1"
      },
      "source": [
        "!pip install preprocess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR_flyCtcuUN"
      },
      "source": [
        "from preprocess import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TiiJwPidRnN"
      },
      "source": [
        "PATH = 'data_in/ChatBotData.csv_short'\n",
        "VOCAB_PATH = 'data_in/vocabulary.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNugyCldpnVa"
      },
      "source": [
        "- 전처리함수를 사용하여 학습할 데이터를 불러오고, 단어 사전을 만든다.\n",
        "   - 전처리 함수는 앞부분에 구현이 되어 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQAUKqeidSvK"
      },
      "source": [
        "inputs, outputs = load_data(PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGj5QE_Rp37C"
      },
      "source": [
        "- 띄어쓰기 단위로 토크나이징한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_86TbZoddbQ"
      },
      "source": [
        "char2idx, idx2char, vocab_size = load_vocabulary(PATH, VOCAB_PATH, tokenize_as_morph=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74Sh2k-kde5P"
      },
      "source": [
        "index_inputs, input_seq_len = enc_processing(inputs, char2idx, tokenize_as_morph=False)\n",
        "index_outputs, output_seq_len = dec_output_processing(outputs, char2idx, tokenize_as_morph=False)\n",
        "index_targets = dec_target_processing(outputs, char2idx, tokenize_as_morph=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGqpPT2Ldg34"
      },
      "source": [
        "data_configs = {}\n",
        "data_configs['char2idx'] = char2idx\n",
        "data_configs['idx2char'] = idx2char\n",
        "data_configs['vocab_size'] = vocab_size\n",
        "data_configs['pad_symbol'] = PAD\n",
        "data_configs['std_symbol'] = STD\n",
        "data_configs['end_symbol'] = END\n",
        "data_configs['unk_symbol'] = UNK"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KJnUTjQdiHN"
      },
      "source": [
        "DATA_IN_PATH = './data_in/'\n",
        "TRAIN_INPUTS = 'train_inputs.npy'\n",
        "TRAIN_OUTPUTS = 'train_outputs.npy'\n",
        "TRAIN_TARGETS = 'train_targets.npy'\n",
        "DATA_CONFIGS = 'data_configs.json'\n",
        "\n",
        "np.save(open(DATA_IN_PATH + TRAIN_INPUTS, 'wb'), index_inputs)\n",
        "np.save(open(DATA_IN_PATH + TRAIN_OUTPUTS , 'wb'), index_outputs)\n",
        "np.save(open(DATA_IN_PATH + TRAIN_TARGETS , 'wb'), index_targets)\n",
        "\n",
        "json.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS, 'w'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPqaMx4QqEos"
      },
      "source": [
        "- char -> idx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgk3cdwqdkBy"
      },
      "source": [
        "char2idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXEkK4FVqIgB"
      },
      "source": [
        "- idx -> char"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVUcx3E0dlI1"
      },
      "source": [
        "idx2char"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuUoy581duv5"
      },
      "source": [
        "## 04 seq2seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bovwSXE_qM-O"
      },
      "source": [
        "- 모델을 구현하기 위한 모듈을 불러온다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc6mdhv-dn4-"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from preprocess import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk9Bf8HUdyVB"
      },
      "source": [
        "- 시각화 함수를 만들어본다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cki9WV-edtLn"
      },
      "source": [
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string], '')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3DDQEJFd1rn"
      },
      "source": [
        "- 학습 데이터 경로 정의\n",
        "  - 상숫값을 깔아놓고 효율적으로 코드를 작성하게 만든다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-Zcbk7Dd38z"
      },
      "source": [
        "DATA_IN_PATH = './data_in/'\n",
        "DATA_OUT_PATH = './data_out/'\n",
        "TRAIN_INPUTS = 'train_inputs.npy'\n",
        "TRAIN_OUTPUTS = 'train_outputs.npy'\n",
        "TRAIN_TARGETS = 'train_targets.npy'\n",
        "DATA_CONFIGS = 'data_configs.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBb4Tuu2d5Id"
      },
      "source": [
        "- 랜덤 시드 고정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G_fv_dHd0uQ"
      },
      "source": [
        "SEED_NUM = 1234\n",
        "tf.random.set_seed(SEED_NUM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAKuyepld8Al"
      },
      "source": [
        "- 파일 로드 : 미리 전처리된 학습에 필요한 데이터와 설정값을 불러온다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrQLQe8sd7XV"
      },
      "source": [
        "index_inputs = np.load(open(DATA_IN_PATH + TRAIN_INPUTS, 'rb'))\n",
        "index_outputs = np.load(open(DATA_IN_PATH + TRAIN_OUTPUTS , 'rb'))\n",
        "index_targets = np.load(open(DATA_IN_PATH + TRAIN_TARGETS , 'rb'))\n",
        "prepro_configs = json.load(open(DATA_IN_PATH + DATA_CONFIGS, 'r'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwJ3wzuJd-1d"
      },
      "source": [
        "# Show length\n",
        "print(len(index_inputs),  len(index_outputs), len(index_targets))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ7d7AqceAxt"
      },
      "source": [
        "- 모델 만들기에 필요한 값 선언"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7589zX-qzvs"
      },
      "source": [
        "- 주요 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mowIY69aeAGj"
      },
      "source": [
        "MODEL_NAME = 'seq2seq_kor'\n",
        "BATCH_SIZE = 2 # 배치 크기\n",
        "MAX_SEQUENCE = 25\n",
        "EPOCH = 30 # 에폭 횟수\n",
        "UNITS = 1024 #재귀 신경망의 결과 차원\n",
        "EMBEDDING_DIM = 256 # 임베딩 차원\n",
        "VALIDATION_SPLIT = 0.1 # 평가셋의 크기 비율\n",
        "\n",
        "char2idx = prepro_configs['char2idx']\n",
        "idx2char = prepro_configs['idx2char']\n",
        "std_index = prepro_configs['std_symbol']\n",
        "end_index = prepro_configs['end_symbol']\n",
        "vocab_size = prepro_configs['vocab_size']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtNezlFdeEsu"
      },
      "source": [
        "- 모델\n",
        "  - 조경현 교수님이 2014년에 발표한 GRU(Gated Recurrent Unit) 모델을 사용하였다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X5mBnPPeGKo"
      },
      "source": [
        "- 인코더"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5SbD07GeEEK"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz # 배치 크기\n",
        "        self.enc_units = enc_units # 재귀 신경망의 결과 차원\n",
        "        self.vocab_size = vocab_size # 사전 크기\n",
        "        self.embedding_dim = embedding_dim # 임베딩 차원         \n",
        "        \n",
        "        # 사전에 포함된 단어를 self.embedding 차원의 임베딩 벡터로 만든다.\n",
        "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        # GRU 신경망을 만드는 부분\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    # 입력값 x 와 은닉 상태 hidden을 받는다. \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)\n",
        "        return output, state\n",
        "\n",
        "    # 배치 크기를 받아 재귀 순환망 초기에 사용될 크기의 은닉 상태를 만든다.\n",
        "    def initialize_hidden_state(self, inp):\n",
        "        return tf.zeros((tf.shape(inp)[0], self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hw358dQJeJtG"
      },
      "source": [
        "- 어텐션\n",
        "  - 어텐션이 추가된 방법은 은닉 상태의 값을 통해 어텐션을 계산하고, 디코더의 각 시퀀스 스텝마다 계산된 어텐션을 입력으로 넣는다.\n",
        "  - 어텐션도 함께 학습을 진행하게 되며 학습을 통해 디코더의 각 시퀀스 스텝마다 어텐션의 가중치를 다르게 적용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrVDZ1XneJE8"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "        # 1차원의 벡터값이 나온다.\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "        # 모델이 중요하다고 판단하는 값은 1에 가까워지고, 영향도가 떨어질수록 0에 가까운 값이 된다.\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfhpiIENeMoY"
      },
      "source": [
        "- 디코더"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3CJEZIueMB0"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.vocab_size = vocab_size \n",
        "        self.embedding_dim = embedding_dim  \n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(self.vocab_size)\n",
        "\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "        \n",
        "    def call(self, x, hidden, enc_output):\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        output, state = self.gru(x)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "            \n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return x, state, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hOf8h9Pvp3T"
      },
      "source": [
        "- 손실 함수와 정확도 측정 함수를 살펴보자.\n",
        "  - 최적화로 아담을 사용한다.\n",
        "  - 크로스 엔트로피로 손실 값을 측정한다.\n",
        "  - 정확도 측정으로 지표를 결정한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poBOCDN8ePAM"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
        "\n",
        "def loss(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "def accuracy(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype), axis=-1)\n",
        "    pred *= mask    \n",
        "    acc = train_accuracy(real, pred)\n",
        "\n",
        "    return tf.reduce_mean(acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iABbO8hteR4V"
      },
      "source": [
        "- 시퀀스 투 시퀀스 모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl7qMKhKeRdV"
      },
      "source": [
        "class seq2seq(tf.keras.Model):\n",
        "    # 인코더와 디코더를 생성할 때 필요하다.\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, dec_units, batch_sz, end_token_idx=2):    \n",
        "        super(seq2seq, self).__init__()\n",
        "        self.end_token_idx = end_token_idx\n",
        "        self.encoder = Encoder(vocab_size, embedding_dim, enc_units, batch_sz) \n",
        "        self.decoder = Decoder(vocab_size, embedding_dim, dec_units, batch_sz) \n",
        "    # 인코더의 입력값과 디코더의 입력값을 x를 통해 받는다.\n",
        "    def call(self, x):\n",
        "        inp, tar = x\n",
        "        \n",
        "        enc_hidden = self.encoder.initialize_hidden_state(inp)\n",
        "        enc_output, enc_hidden = self.encoder(inp, enc_hidden)\n",
        "\n",
        "        dec_hidden = enc_hidden\n",
        "\n",
        "        predict_tokens = list()\n",
        "        for t in range(0, tar.shape[1]):\n",
        "            dec_input = tf.dtypes.cast(tf.expand_dims(tar[:, t], 1), tf.float32) \n",
        "            predictions, dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_output)\n",
        "            predict_tokens.append(tf.dtypes.cast(predictions, tf.float32))   \n",
        "        return tf.stack(predict_tokens, axis=1)\n",
        "    # 사용자의 입력에 대한 모델의 결괏값을 확인하기 위해 테스트 목적으로 만들어진 함수\n",
        "    def inference(self, x):\n",
        "        inp  = x\n",
        "\n",
        "        enc_hidden = self.encoder.initialize_hidden_state(inp)\n",
        "        enc_output, enc_hidden = self.encoder(inp, enc_hidden)\n",
        "\n",
        "        dec_hidden = enc_hidden\n",
        "        \n",
        "        dec_input = tf.expand_dims([char2idx[std_index]], 1)\n",
        "        \n",
        "        predict_tokens = list()\n",
        "        for t in range(0, MAX_SEQUENCE):\n",
        "            predictions, dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_output)\n",
        "            predict_token = tf.argmax(predictions[0])\n",
        "            \n",
        "            if predict_token == self.end_token_idx:\n",
        "                break\n",
        "            \n",
        "            predict_tokens.append(predict_token)\n",
        "            dec_input = tf.dtypes.cast(tf.expand_dims([predict_token], 0), tf.float32)   \n",
        "            \n",
        "        return tf.stack(predict_tokens, axis=0).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSMdrT1rwiJa"
      },
      "source": [
        "- seq2seq를 만들어본다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFl3aDSjeVBM"
      },
      "source": [
        "model = seq2seq(vocab_size, EMBEDDING_DIM, UNITS, UNITS, BATCH_SIZE, char2idx[end_index])\n",
        "model.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(1e-3), metrics=[accuracy])\n",
        "#model.run_eagerly = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4k9BD5AeXP-"
      },
      "source": [
        "- 학습 진행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoJWOUNAeWVT"
      },
      "source": [
        "# 체크포인트가 저장된 폴더를 만든다.\n",
        "PATH = DATA_OUT_PATH + MODEL_NAME\n",
        "if not(os.path.isdir(PATH)):\n",
        "        os.makedirs(os.path.join(PATH))\n",
        "        \n",
        "checkpoint_path = DATA_OUT_PATH + MODEL_NAME + '/weights.h5'\n",
        "\n",
        "# 두 개의 함수를 정의한다.\n",
        "# 모델 체크포인트를 저장할 방법에 대한 함수    \n",
        "cp_callback = ModelCheckpoint(\n",
        "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "# 학습을 빨리 끝내기 위한 earlystop_callback 함수\n",
        "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=10)\n",
        "\n",
        "history = model.fit([index_inputs, index_outputs], index_targets,\n",
        "                    batch_size=BATCH_SIZE, epochs=EPOCH,\n",
        "                    validation_split=VALIDATION_SPLIT, callbacks=[earlystop_callback, cp_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9B8ysRGeb4d"
      },
      "source": [
        "- 결과 플롯"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwOrjkMPw8nc"
      },
      "source": [
        "- 정확도를 판단한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7V_ej-oeZxg"
      },
      "source": [
        "plot_graphs(history, 'accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms2kqkQ4w-QZ"
      },
      "source": [
        "- 평가 손실값을 판단한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTutjsjpeeEg"
      },
      "source": [
        "plot_graphs(history, 'loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sczv1eavefxW"
      },
      "source": [
        "- 결과 확인\n",
        "  - 저장된 모델을 로드하는 방법을 보자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rH5zugkefM0"
      },
      "source": [
        "SAVE_FILE_NM = \"weights.h5\"\n",
        "model.load_weights(os.path.join(DATA_OUT_PATH, MODEL_NAME, SAVE_FILE_NM))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPxacUuzxT04"
      },
      "source": [
        "- 학습이 잘 되었는지 확인해본다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp1DKz1VeiG7"
      },
      "source": [
        "query = \"남자친구 승진 선물로 뭐가 좋을까?\"\n",
        "\n",
        "test_index_inputs, _ = enc_processing([query], char2idx)    \n",
        "predict_tokens = model.inference(test_index_inputs)\n",
        "print(predict_tokens)\n",
        "\n",
        "print(' '.join([idx2char[str(t)] for t in predict_tokens]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUTCGtbjxk47"
      },
      "source": [
        "query = \"나 오늘 소개팅하러가\"\n",
        "\n",
        "test_index_inputs, _ = enc_processing([query], char2idx)    \n",
        "predict_tokens = model.inference(test_index_inputs)\n",
        "print(predict_tokens)\n",
        "\n",
        "print(' '.join([idx2char[str(t)] for t in predict_tokens]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEe6Wg16xsvs"
      },
      "source": [
        "query = \"나 오늘 너무 힘들었잖아\"\n",
        "\n",
        "test_index_inputs, _ = enc_processing([query], char2idx)    \n",
        "predict_tokens = model.inference(test_index_inputs)\n",
        "print(predict_tokens)\n",
        "\n",
        "print(' '.join([idx2char[str(t)] for t in predict_tokens]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYbilqtZxXra"
      },
      "source": [
        "- 시퀀스 투 시퀀스 모델에 어텐션 기법을 추가한 모델을 만들어 챗봇 기능을 구현했다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeYMCimwepgj"
      },
      "source": [
        "## 05 transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYskEm4exyiY"
      },
      "source": [
        "- 트랜스포머 : 어텐션 구조만으로 전체 모델을 만들어 어텐션 기법의 중요성을 강조한다.\n",
        "- 문장의 길이가 길어진 경우 순환 신경망의 한계를 해결하는 기법이 트랜스포머다.\n",
        "- 각 단어 간의 유의미한 관계를 잡아내는 것도 트랜스포머가 기존 순환 신경망 기반의 시퀀스 투 시퀀스 모델보다 뛰어나다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYTi49Pqych0"
      },
      "source": [
        "### 셀프 어텐션\n",
        "- 문장에서 각 단어끼리 얼마나 관계가 있는지를 계산해서 반영하는 방법\n",
        "- 각 단어를 기준으로 다른 단어들과의 관계 값을 계산하는 것이 어텐션 스코어\n",
        "- 어텐션 스코어 값을 하나의 테이블로 만든 것을 어텐션 맵\n",
        "- 트랜스포머 모델에서는 단어 벡터끼리 내적 연산을 함으로써 어텐션 스코어를 구하였다.\n",
        "- 이렇게 구한 어텐션 스코어에 대해 소프트맥스를 적용한 후 각 벡터와 곱한 후 전체 벡터를 더해서 해당 단어에 대해 문맥 벡터를 구한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCjNZ-q1etMl"
      },
      "source": [
        "- 모듈 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YEA7ZNFevxq"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from konlpy.tag import Twitter\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import enum\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from preprocess import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH5XFXNbewnY"
      },
      "source": [
        "- 시각화 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usbpGoxkewHw"
      },
      "source": [
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string], '')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ9XTchRezgF"
      },
      "source": [
        "- 학습 데이터 경로 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9TBrcl_eyuV"
      },
      "source": [
        "DATA_IN_PATH = './data_in/'\n",
        "DATA_OUT_PATH = './data_out/'\n",
        "TRAIN_INPUTS = 'train_inputs.npy'\n",
        "TRAIN_OUTPUTS = 'train_outputs.npy'\n",
        "TRAIN_TARGETS = 'train_targets.npy'\n",
        "DATA_CONFIGS = 'data_configs.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvCR2MnSe31y"
      },
      "source": [
        "- 랜덤 시드 고정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGZgUuR5e24W"
      },
      "source": [
        "SEED_NUM = 1234\n",
        "tf.random.set_seed(SEED_NUM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22_Wl235e6ci"
      },
      "source": [
        "- 파일 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_NyXAdVe572"
      },
      "source": [
        "index_inputs = np.load(open(DATA_IN_PATH + TRAIN_INPUTS, 'rb'))\n",
        "index_outputs = np.load(open(DATA_IN_PATH + TRAIN_OUTPUTS , 'rb'))\n",
        "index_targets = np.load(open(DATA_IN_PATH + TRAIN_TARGETS , 'rb'))\n",
        "prepro_configs = json.load(open(DATA_IN_PATH + DATA_CONFIGS, 'r'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBcnFCUBe9Fb"
      },
      "source": [
        "- 모델 하이퍼파라미터 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC9e6kFZe8n0"
      },
      "source": [
        "char2idx = prepro_configs['char2idx']\n",
        "end_index = prepro_configs['end_symbol']\n",
        "model_name = 'transformer'\n",
        "vocab_size = prepro_configs['vocab_size']\n",
        "BATCH_SIZE = 2\n",
        "MAX_SEQUENCE = 25\n",
        "EPOCHS = 30\n",
        "VALID_SPLIT = 0.1\n",
        "\n",
        "kargs = {'model_name': model_name,\n",
        "         'num_layers': 2,\n",
        "         'd_model': 512,\n",
        "         'num_heads': 8,\n",
        "         'dff': 2048,\n",
        "         'input_vocab_size': vocab_size,\n",
        "         'target_vocab_size': vocab_size,\n",
        "         'maximum_position_encoding': MAX_SEQUENCE,\n",
        "         'end_token_idx': char2idx[end_index],\n",
        "         'rate': 0.1\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIPagiV1zw27"
      },
      "source": [
        "### 트랜스포머 네트워크의 전체 구조\n",
        "- multi-head attention\n",
        "- subsequent masked attention\n",
        "- position-wise feed forward network\n",
        "- residual connection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJziE9bT1EaM"
      },
      "source": [
        "#### 멀티 헤드 어텐션\n",
        "1. 스케일 내적 어텐션     \n",
        "  중간에 크기를 조정하는 과정이 추가된 것이다.\n",
        "2. 순방향 마스크 어텐션     \n",
        "  자신보다 뒤에 있는 단어를 참고하지 않게 하는 기법    \n",
        "  소프트맥스 함수에 매우 작은 음숫값을 넣는 경우 거의 0 값에 수렴하는 값을 갖게 된다.\n",
        "3. 멀티 헤드 어텐션\n",
        "  어텐션 맵을 여럿 만들어 다양한 특징에 대한 어텐션을 볼 수 있게 한 방법이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzAxgqjBfATE"
      },
      "source": [
        "- 모델 선언 및 컴파일\n",
        "  - 패딩 및 포워드 마스킹"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nPqVHBOe_qf"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "    # add extra dimensions to add the padding\n",
        "    # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe--ZAO5fEiv"
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KcVmDGkfG2q"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by \n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uvL9_XMfIhB"
      },
      "source": [
        "enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(index_inputs, index_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S20aAXK93ECc"
      },
      "source": [
        "#### 레지듀얼 커넥션"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWrkhzuUfKHO"
      },
      "source": [
        "#### 포지셔널 인코딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZelHs92fJrQ"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * i//2) / np.float32(d_model))\n",
        "    return pos * angle_rates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs_lbF1ZfMdp"
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWhbTUi0fNr1"
      },
      "source": [
        "pos_encoding = positional_encoding(50, 512)\n",
        "print (pos_encoding.shape)\n",
        "\n",
        "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
        "plt.xlabel('Depth')\n",
        "plt.xlim((0, 512))\n",
        "plt.ylabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCDJfByJfQOo"
      },
      "source": [
        "- 어텐션"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L9M-UxUfPDY"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead) \n",
        "    but it must be broadcastable for addition.\n",
        "\n",
        "    Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "    output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxwYjPmFfTFN"
      },
      "source": [
        "- 멀티헤드 어텐션\n",
        "  - 어텐션 맵을 여럿 만들어 다양한 특징에 대한 어텐션을 볼 수 있게 한 방법이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vlg_w4VXfSjd"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kargs):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = kargs['num_heads']\n",
        "        self.d_model = kargs['d_model']\n",
        "\n",
        "        # 나머지가 발생하면 안된다.\n",
        "        assert self.d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = self.d_model // self.num_heads\n",
        "\n",
        "        # 스케일 내적 연산 이전에 입력한 입력한 key, query, value에 대한 차원 수를 맞추기 위한 레이어다.\n",
        "        self.wq = tf.keras.layers.Dense(kargs['d_model'])\n",
        "        self.wk = tf.keras.layers.Dense(kargs['d_model'])\n",
        "        self.wv = tf.keras.layers.Dense(kargs['d_model'])\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(kargs['d_model'])\n",
        "\n",
        "    # key, query, value에 대한 벡터를 헤드 수만큼 분리할 수 있게 해주는 함수다.\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, \n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdPVpOZGfWG6"
      },
      "source": [
        "#### 포지션-와이즈 피드포워드 네트워크\n",
        "- 위 수식과 같이 네트워크를 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZofOh9T6fVkZ"
      },
      "source": [
        "def point_wise_feed_forward_network(**kargs):\n",
        "    return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(kargs['dff'], activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(kargs['d_model'])  # (batch_size, seq_len, d_model)\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgbQ9R5ufaEg"
      },
      "source": [
        "- 인코더 레이어"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-sudwYBfZky"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kargs):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(**kargs)\n",
        "        self.ffn = point_wise_feed_forward_network(**kargs)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(kargs['rate'])\n",
        "        self.dropout2 = tf.keras.layers.Dropout(kargs['rate'])\n",
        "\n",
        "    def call(self, x, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jbiglsssfc4b"
      },
      "source": [
        "- 디코더 레이어"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmhouBKCfceY"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kargs):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(**kargs)\n",
        "        self.mha2 = MultiHeadAttention(**kargs)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(**kargs)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(kargs['rate'])\n",
        "        self.dropout2 = tf.keras.layers.Dropout(kargs['rate'])\n",
        "        self.dropout3 = tf.keras.layers.Dropout(kargs['rate'])\n",
        "    \n",
        "    \n",
        "    def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn2 = self.dropout2(attn2)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output)\n",
        "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98p0qBurffy1"
      },
      "source": [
        "- 인코더"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovUVgFBrffVc"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kargs):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = kargs['d_model']\n",
        "        self.num_layers = kargs['num_layers']\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(kargs['input_vocab_size'], self.d_model)\n",
        "        self.pos_encoding = positional_encoding(kargs['maximum_position_encoding'], \n",
        "                                                self.d_model)\n",
        "\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(**kargs) \n",
        "                           for _ in range(self.num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(kargs['rate'])\n",
        "\n",
        "    def call(self, x, mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # adding embedding and position encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y5hNmSsfiyU"
      },
      "source": [
        "- 디코더"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UfmofdSfiRr"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kargs):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = kargs['d_model']\n",
        "        self.num_layers = kargs['num_layers']\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(kargs['target_vocab_size'], self.d_model)\n",
        "        self.pos_encoding = positional_encoding(kargs['maximum_position_encoding'], self.d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(**kargs) \n",
        "                           for _ in range(self.num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(kargs['rate'])\n",
        "\n",
        "    def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABnwMJU_flyL"
      },
      "source": [
        "- 트랜스포머 모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u69NtUPaflUq"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, **kargs):\n",
        "        super(Transformer, self).__init__(name=kargs['model_name'])\n",
        "        self.end_token_idx = kargs['end_token_idx']\n",
        "        \n",
        "        self.encoder = Encoder(**kargs)\n",
        "        self.decoder = Decoder(**kargs)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(kargs['target_vocab_size'])\n",
        "\n",
        "    def call(self, x):\n",
        "        inp, tar = x\n",
        "\n",
        "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
        "        enc_output = self.encoder(inp, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, _ = self.decoder(\n",
        "            tar, enc_output, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "        return final_output\n",
        "    \n",
        "    def inference(self, x):\n",
        "        inp = x\n",
        "        tar = tf.expand_dims([STD_INDEX], 0)\n",
        "\n",
        "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)        \n",
        "        enc_output = self.encoder(inp, enc_padding_mask)\n",
        "        \n",
        "        predict_tokens = list()\n",
        "        for t in range(0, MAX_SEQUENCE):\n",
        "            dec_output, _ = self.decoder(tar, enc_output, look_ahead_mask, dec_padding_mask)\n",
        "            final_output = self.final_layer(dec_output)\n",
        "            outputs = tf.argmax(final_output, -1).numpy()\n",
        "            pred_token = outputs[0][-1]\n",
        "            if pred_token == self.end_token_idx:\n",
        "                break\n",
        "            predict_tokens.append(pred_token)\n",
        "            tar = tf.expand_dims([STD_INDEX] + predict_tokens, 0)\n",
        "            _, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
        "            \n",
        "        return predict_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU_8x4lJfo3C"
      },
      "source": [
        "- 모델 로스 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF5dwHq7foeL"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
        "\n",
        "def loss(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "def accuracy(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype), axis=-1)\n",
        "    pred *= mask    \n",
        "    acc = train_accuracy(real, pred)\n",
        "\n",
        "    return tf.reduce_mean(acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlOoZ7WCfrPN"
      },
      "source": [
        "model = Transformer(**kargs)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              loss=loss,\n",
        "              metrics=[accuracy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eW7Lid7ftdc"
      },
      "source": [
        "- Callback 선언"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA5xcZw4fslw"
      },
      "source": [
        "# overfitting을 막기 위한 ealrystop 추가\n",
        "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=10)\n",
        "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n",
        "# patience: no improvment epochs (patience = 1, 1번 이상 상승이 없으면 종료)\n",
        "\n",
        "checkpoint_path = DATA_OUT_PATH + model_name + '/weights.h5'\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create path if exists\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
        "else:\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
        "    \n",
        "\n",
        "cp_callback = ModelCheckpoint(\n",
        "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfiGHDoFfxd0"
      },
      "source": [
        "- 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lboQnbZ3fv13"
      },
      "source": [
        "history = model.fit([index_inputs, index_outputs], index_targets, \n",
        "                    batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "                    validation_split=VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFRsR6M5f6eJ"
      },
      "source": [
        "- 결과 플롯"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf3JJu0Cfzab"
      },
      "source": [
        "plot_graphs(history, 'accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1hynFBCf8ft"
      },
      "source": [
        "plot_graphs(history, 'loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq4GCal5f-fA"
      },
      "source": [
        "- 베스트 모델 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_Xr5Uqwf9s_"
      },
      "source": [
        "DATA_OUT_PATH = './data_out/'\n",
        "SAVE_FILE_NM = 'weights.h5'\n",
        "\n",
        "model.load_weights(os.path.join(DATA_OUT_PATH, model_name, SAVE_FILE_NM))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cheMpHqqgCI-"
      },
      "source": [
        "- 모델 결과 출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0ZYp1cNgA70"
      },
      "source": [
        "char2idx = prepro_configs['char2idx']\n",
        "idx2char = prepro_configs['idx2char']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQ8E9AMUgDnB"
      },
      "source": [
        "text = \"남자친구 승진 선물로 뭐가 좋을까?\"\n",
        "test_index_inputs, _ = enc_processing([text], char2idx)\n",
        "outputs = model.inference(test_index_inputs)\n",
        "\n",
        "print(' '.join([idx2char[str(o)] for o in outputs]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9EUkkkkgFEG"
      },
      "source": [
        "text = \"나 돈이 없어\"\n",
        "test_index_inputs, _ = enc_processing([text], char2idx)\n",
        "outputs = model.inference(test_index_inputs)\n",
        "\n",
        "print(' '.join([idx2char[str(o)] for o in outputs]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMd0ckw2gURz"
      },
      "source": [
        "- 아쉬운 결과"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyI62L1qgGiQ"
      },
      "source": [
        "text = \"난 오늘 밤 사람을 죽이겠어\"\n",
        "test_index_inputs, _ = enc_processing([text], char2idx)\n",
        "outputs = model.inference(test_index_inputs)\n",
        "\n",
        "print(' '.join([idx2char[str(o)] for o in outputs]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goV_KR-agI_J"
      },
      "source": [
        "text = \"사람의 몸에 칼을 꽂아보고 싶어\"\n",
        "test_index_inputs, _ = enc_processing([text], char2idx)\n",
        "outputs = model.inference(test_index_inputs)\n",
        "\n",
        "print(' '.join([idx2char[str(o)] for o in outputs]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU25vFoIgSxM"
      },
      "source": [
        "text = \"컴퓨터에 불을 지르고 싶어\"\n",
        "test_index_inputs, _ = enc_processing([text], char2idx)\n",
        "outputs = model.inference(test_index_inputs)\n",
        "\n",
        "print(' '.join([idx2char[str(o)] for o in outputs]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWy-5CG3ge5-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}